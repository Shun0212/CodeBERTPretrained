{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNfQSk8KW0MEtdWk48p8Y/z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shun0212/CodeBERTPretrained/blob/main/EvalCodeMor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZH5_cdm9-J0Y",
        "outputId": "1d2844b6-993e-46a3-b16b-cd89dfdd8c7b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# デバイス設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用デバイス: {device}\")\n",
        "\n",
        "def get_cls_embedding(model, tokenizer, text, device, max_length=256):\n",
        "    \"\"\"\n",
        "    入力テキストから [CLS] トークンの埋め込みを取得する関数。\n",
        "    ※モデルが BERT 系の場合は model.bert を、RoBERTa 系の場合は model.roberta を利用して出力を取得します。\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # モデルの種類に応じて内部のエンコーダにアクセスする\n",
        "    if hasattr(model, \"bert\"):\n",
        "        outputs = model.bert(**inputs)\n",
        "    elif hasattr(model, \"roberta\"):\n",
        "        outputs = model.roberta(**inputs)\n",
        "    else:\n",
        "        raise ValueError(\"Model does not have attribute 'bert' or 'roberta'.\")\n",
        "\n",
        "    # outputs.last_hidden_state: (batch_size, sequence_length, hidden_size)\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] または最初のトークンの埋め込み\n",
        "    return cls_embedding.detach().cpu().numpy()\n",
        "\n",
        "def calculate_metrics(sim_matrix):\n",
        "    \"\"\"\n",
        "    類似度行列から MRR, Recall@k, Precision@k などの評価指標を計算する関数。\n",
        "    \"\"\"\n",
        "    num_examples = sim_matrix.shape[0]\n",
        "    mrr_total = 0.0\n",
        "    recall_at_1_total = 0.0\n",
        "    recall_at_5_total = 0.0\n",
        "    precision_at_1_total = 0.0\n",
        "    precision_at_5_total = 0.0\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        sims = sim_matrix[i]\n",
        "        ranked_indices = np.argsort(-sims)\n",
        "        rank = np.where(ranked_indices == i)[0][0] + 1\n",
        "        mrr_total += 1.0 / rank\n",
        "\n",
        "        # Recall@k, Precision@k の計算\n",
        "        if i in ranked_indices[:1]:\n",
        "            recall_at_1_total += 1.0\n",
        "            precision_at_1_total += 1.0\n",
        "        if i in ranked_indices[:5]:\n",
        "            recall_at_5_total += 1.0\n",
        "            precision_at_5_total += 1.0\n",
        "\n",
        "    mrr = mrr_total / num_examples\n",
        "    recall_at_1 = recall_at_1_total / num_examples\n",
        "    recall_at_5 = recall_at_5_total / num_examples\n",
        "    precision_at_1 = precision_at_1_total / num_examples\n",
        "    precision_at_5 = precision_at_5_total / num_examples\n",
        "\n",
        "    metrics = {\n",
        "        \"mrr\": mrr,\n",
        "        \"recall@1\": recall_at_1,\n",
        "        \"recall@5\": recall_at_5,\n",
        "        \"precision@1\": precision_at_1,\n",
        "        \"precision@5\": precision_at_5,\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def evaluate_code_search(model, tokenizer, dataset, device, max_examples=100):\n",
        "    \"\"\"\n",
        "    CodeSearchNet の一部サンプルを使って、クエリからコード検索の評価を行い、MRR, Recall@k, Precision@k を計算する関数。\n",
        "\n",
        "    ... ( Docstring は変更なし ) ...\n",
        "    \"\"\"\n",
        "    codes = []\n",
        "    queries = []\n",
        "    num_examples = min(len(dataset), max_examples)\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        ex = dataset[i]\n",
        "        # キーがどの名前になっているか確認して抽出する\n",
        "        if \"func_code_string\" in ex:\n",
        "            code_text = ex[\"func_code_string\"]\n",
        "        elif \"code\" in ex:\n",
        "            code_text = ex[\"code\"]\n",
        "        else:\n",
        "            raise KeyError(f\"サンプル {i} にコードテキストが見つかりません。利用可能なキー: {list(ex.keys())}\")\n",
        "\n",
        "        if \"func_documentation_string\" in ex:\n",
        "            query_text = ex[\"func_documentation_string\"]\n",
        "        elif \"docstring\" in ex:\n",
        "            query_text = ex[\"docstring\"]\n",
        "        else:\n",
        "            raise KeyError(f\"サンプル {i} にドキュメンテーション文字列が見つかりません。利用可能なキー: {list(ex.keys())}\")\n",
        "\n",
        "        codes.append(code_text)\n",
        "        queries.append(query_text)\n",
        "\n",
        "    # 各コードとクエリの埋め込みを取得\n",
        "    code_embeddings = []\n",
        "    query_embeddings = []\n",
        "\n",
        "    for code in codes:\n",
        "        emb = get_cls_embedding(model, tokenizer, code, device)\n",
        "        code_embeddings.append(emb)\n",
        "    for query in queries:\n",
        "        emb = get_cls_embedding(model, tokenizer, query, device)\n",
        "        query_embeddings.append(emb)\n",
        "\n",
        "    # 各埋め込みは shape (1, hidden_size) になっているので連結\n",
        "    code_embeddings = np.concatenate(code_embeddings, axis=0)  # shape: (num_examples, hidden_size)\n",
        "    query_embeddings = np.concatenate(query_embeddings, axis=0)\n",
        "\n",
        "    # クエリとコードの間の cosine similarity 行列を計算\n",
        "    sim_matrix = cosine_similarity(query_embeddings, code_embeddings)\n",
        "\n",
        "    # 評価指標の計算\n",
        "    metrics = calculate_metrics(sim_matrix)\n",
        "    return metrics\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # CodeSearchNet の Python 部分のテストセットをロード\n",
        "    print(\"CodeSearchNet データセットをロードします...\")\n",
        "    dataset = load_dataset(\"code_search_net\", \"python\", split=\"test\", trust_remote_code=True)\n",
        "    # デモ用に上位 100 件を評価対象にする\n",
        "    subset = dataset.select(range(1000))\n",
        "\n",
        "    ### CodeMorph-BERT の評価 ###\n",
        "    model_name1 = \"Shuu12121/CodeMorph-BERT\"\n",
        "    print(f\"\\n{model_name1} を評価します...\")\n",
        "    tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n",
        "    model1 = AutoModelForMaskedLM.from_pretrained(model_name1)\n",
        "    model1.to(device)\n",
        "    metrics1 = evaluate_code_search(model1, tokenizer1, subset, device, max_examples=23107) # max_examples は適宜調整\n",
        "    print(f\"CodeMorph-BERT の評価指標:\")\n",
        "    for name, value in metrics1.items():\n",
        "        print(f\"  {name}: {value:.4f}\")\n",
        "\n",
        "    ### Microsoft CodeBERT の評価 ###\n",
        "    model_name2 = \"microsoft/codebert-base-mlm\"\n",
        "    print(f\"\\n{model_name2} を評価します...\")\n",
        "    tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
        "    model2 = AutoModelForMaskedLM.from_pretrained(model_name2)\n",
        "    model2.to(device)\n",
        "    metrics2 = evaluate_code_search(model2, tokenizer2, subset, device, max_examples=23107) # max_examples は適宜調整\n",
        "    print(f\"CodeBERT の評価指標:\")\n",
        "    for name, value in metrics2.items():\n",
        "        print(f\"  {name}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NK0JTEvJ1dHc",
        "outputId": "50378dd7-6c0f-464b-9cf3-425ba97c5f79"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用デバイス: cuda\n",
            "CodeSearchNet データセットをロードします...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shuu12121/CodeMorph-BERT を評価します...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CodeMorph-BERT の評価指標:\n",
            "  mrr: 0.7668\n",
            "  recall@1: 0.6930\n",
            "  recall@5: 0.8500\n",
            "  precision@1: 0.6930\n",
            "  precision@5: 0.8500\n",
            "\n",
            "microsoft/codebert-base-mlm を評価します...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/codebert-base-mlm were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CodeBERT の評価指標:\n",
            "  mrr: 0.6766\n",
            "  recall@1: 0.6180\n",
            "  recall@5: 0.7370\n",
            "  precision@1: 0.6180\n",
            "  precision@5: 0.7370\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer, RobertaForMaskedLM, BertForMaskedLM\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "\n",
        "# デバイス設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用デバイス: {device}\")\n",
        "\n",
        "def get_cls_embedding(model, tokenizer, text, device, max_length=256):\n",
        "    \"\"\"\n",
        "    入力テキストから [CLS] トークンの埋め込みを取得する関数。\n",
        "    ※モデルが BERT 系の場合は model.bert を、RoBERTa 系の場合は model.roberta を利用して出力を取得します。\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # モデルの種類に応じて内部のエンコーダにアクセスする\n",
        "    if isinstance(model, BertForMaskedLM): # BERT 系モデルの判定を修正\n",
        "        outputs = model.bert(**inputs)\n",
        "    elif isinstance(model, RobertaForMaskedLM): # RoBERTa 系モデルの判定を修正\n",
        "        outputs = model.roberta(**inputs)\n",
        "    else:\n",
        "        raise ValueError(\"Model must be BertForMaskedLM or RobertaForMaskedLM.\") # エラーメッセージを修正\n",
        "\n",
        "    # outputs.last_hidden_state: (batch_size, sequence_length, hidden_size)\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] または最初のトークンの埋め込み\n",
        "    return cls_embedding.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def calculate_metrics(sim_matrix, k_values=[1, 5, 10, 50, 100]):\n",
        "    num_queries = sim_matrix.shape[0]\n",
        "    metrics = {\n",
        "        \"mrr\": 0.0,\n",
        "        \"recall@k\": {k: 0.0 for k in k_values},\n",
        "        \"precision@k\": {k: 0.0 for k in k_values},\n",
        "        \"ndcg@k\": {k: 0.0 for k in k_values},\n",
        "        \"map\": 0.0,\n",
        "        \"f1@k\": {k: 0.0 for k in k_values},\n",
        "        \"r_precision\": 0.0,\n",
        "        \"success_rate@k\": {k: 0.0 for k in k_values},\n",
        "        \"query_coverage@k\": {k: 0.0 for k in k_values},\n",
        "    }\n",
        "\n",
        "    for i in range(num_queries):\n",
        "        sims = sim_matrix[i]\n",
        "        ranked_indices = np.argsort(-sims)\n",
        "        correct_rank = np.where(ranked_indices == 0)[0][0] + 1\n",
        "        is_correct_in_top_k = {k: correct_rank <= k for k in k_values}\n",
        "\n",
        "        # MRR\n",
        "        metrics[\"mrr\"] += 1.0 / correct_rank\n",
        "\n",
        "        # 各 k に対してクエリごとの precision, recall, f1 を計算\n",
        "        for k in k_values:\n",
        "            if correct_rank <= k:\n",
        "                # 正解がトップkに含まれる場合\n",
        "                prec = 1.0 / correct_rank\n",
        "                rec = 1.0  # 候補が1件の場合、正解が含まれていれば recall は 1.0\n",
        "            else:\n",
        "                prec = 0.0\n",
        "                rec = 0.0\n",
        "            f1 = calculate_f1(prec, rec)\n",
        "\n",
        "            metrics[\"precision@k\"][k] += prec\n",
        "            metrics[\"recall@k\"][k] += rec\n",
        "            metrics[\"f1@k\"][k] += f1\n",
        "\n",
        "            # NDCG\n",
        "            ideal_ranking = [1.0] + [0.0] * (k - 1)\n",
        "            actual_ranking = [1.0 if j == 0 else 0.0 for j in ranked_indices[:k]]\n",
        "            idcg = calculate_dcg(ideal_ranking)\n",
        "            dcg = calculate_dcg(actual_ranking)\n",
        "            metrics[\"ndcg@k\"][k] += dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "            # Success Rate, Query Coverage\n",
        "            metrics[\"success_rate@k\"][k] += 1.0 if is_correct_in_top_k[k] else 0.0\n",
        "            metrics[\"query_coverage@k\"][k] += 1.0 if is_correct_in_top_k[k] else 0.0\n",
        "\n",
        "        # MAP, R-Precision\n",
        "        metrics[\"map\"] += calculate_average_precision(ranked_indices)\n",
        "        metrics[\"r_precision\"] += calculate_r_precision(ranked_indices)\n",
        "\n",
        "    # 各指標をクエリ数で平均化\n",
        "    metrics[\"mrr\"] /= num_queries\n",
        "    metrics[\"map\"] /= num_queries\n",
        "    metrics[\"r_precision\"] /= num_queries\n",
        "    for k in k_values:\n",
        "        metrics[\"recall@k\"][k] /= num_queries\n",
        "        metrics[\"precision@k\"][k] /= num_queries\n",
        "        metrics[\"ndcg@k\"][k] /= num_queries\n",
        "        metrics[\"f1@k\"][k] /= num_queries\n",
        "        metrics[\"success_rate@k\"][k] /= num_queries\n",
        "        metrics[\"query_coverage@k\"][k] /= num_queries\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def calculate_average_precision(ranked_indices): # ★ MAP 計算用関数を追加\n",
        "    \"\"\"\n",
        "    Average Precision (AP) を計算する関数。\n",
        "    \"\"\"\n",
        "    correct_rank = np.where(ranked_indices == 0)[0][0] + 1\n",
        "    ap = 0.0\n",
        "    for k in range(1, len(ranked_indices) + 1):\n",
        "        if k == correct_rank: # k番目に正解コードが現れた場合のみ Precision を加算\n",
        "            ap += 1.0 / k\n",
        "    return ap\n",
        "\n",
        "\n",
        "def calculate_f1(precision, recall): # ★ F1値計算用関数を追加\n",
        "    \"\"\"\n",
        "    F1値を計算する関数 (Precision と Recall から計算)。\n",
        "    \"\"\"\n",
        "    if precision + recall == 0: # Precision, Recall が共に 0 の場合\n",
        "        return 0.0\n",
        "    return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "\n",
        "def calculate_r_precision(ranked_indices): # ★ R-Precision 計算用関数を追加\n",
        "    \"\"\"\n",
        "    R-Precision を計算する関数 (R=1 で固定: 候補プールに正解コードは1つのみ)。\n",
        "    \"\"\"\n",
        "    r = 1 # 正解コード数 (候補プールに1つのみ)\n",
        "    correct_in_top_r = 0\n",
        "    for i in range(r): # 上位 r 件まで確認\n",
        "        if ranked_indices[i] == 0: # 正解コードがランクイン\n",
        "            correct_in_top_r += 1\n",
        "    return correct_in_top_r / r # R-Precision は適合率\n",
        "\n",
        "def calculate_dcg(ranking):\n",
        "    \"\"\"\n",
        "    Discounted Cumulative Gain (DCG) を計算する関数。\n",
        "    \"\"\"\n",
        "    dcg = 0.0\n",
        "    for i, rel in enumerate(ranking):\n",
        "        dcg += rel / np.log2(i + 2) # 順位 i+1 の割引率: 1/log2(i+2)\n",
        "    return dcg\n",
        "\n",
        "\n",
        "def evaluate_code_search(model, tokenizer, dataset, device, max_examples=100, pool_size=100):\n",
        "    \"\"\"\n",
        "    CodeSearchNet データセットを使って、クエリからコード検索の厳密な評価を行い、\n",
        "    MRR, Recall@k, Precision@k, NDCG@k を計算する関数 (候補コードプール使用)。\n",
        "\n",
        "    Args:\n",
        "        model, tokenizer: 評価対象のモデルとトークナイザー\n",
        "        dataset: Hugging Face Datasets の CodeSearchNet データセット\n",
        "        device: \"cpu\" または \"cuda\"\n",
        "        max_examples: 評価に使うクエリの最大数\n",
        "        pool_size: 各クエリに対する候補コードプールのサイズ (正解コード1つ + 不正解コード pool_size-1)\n",
        "\n",
        "    Returns:\n",
        "        metrics: 評価指標 (MRR, Recall@k, Precision@k, NDCG@k) を含む辞書\n",
        "    \"\"\"\n",
        "    num_examples = min(len(dataset), max_examples)\n",
        "    all_codes = [dataset[i][\"func_code_string\"] for i in range(len(dataset))] # データセット全体のコードを取得 (不正解候補サンプリング用)\n",
        "    queries = []\n",
        "    correct_code_indices = [] # 各クエリに対応する正解コードのデータセット内インデックスを保持\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        ex = dataset[i]\n",
        "        query_text = ex[\"func_documentation_string\"]\n",
        "        queries.append(query_text)\n",
        "        correct_code_indices.append(i) # 正解コードのインデックスを記録\n",
        "\n",
        "    # データセット全体のコード埋め込みを事前に計算 (時間短縮のため)\n",
        "    all_code_embeddings = []\n",
        "    print(\"データセット全体のコード埋め込みを計算...\")\n",
        "    for code in all_codes[:num_examples]: # max_examples までに制限\n",
        "        emb = get_cls_embedding(model, tokenizer, code, device)\n",
        "        all_code_embeddings.append(emb)\n",
        "    all_code_embeddings = np.concatenate(all_code_embeddings, axis=0) # shape: (num_examples, hidden_size)\n",
        "    print(\"コード埋め込み計算完了.\")\n",
        "\n",
        "\n",
        "    sim_matrices = [] # 各クエリの類似度行列を格納するリスト\n",
        "    print(\"候補コードプールを作成し、類似度行列を計算...\")\n",
        "    for i in range(num_examples): # 各クエリごとに候補コードプールを作成\n",
        "        query_embedding = get_cls_embedding(model, tokenizer, queries[i], device).reshape(1, -1) # クエリ埋め込みを計算 # ★reshape を追加\n",
        "\n",
        "        candidate_pool_embeddings = []\n",
        "        candidate_pool_embeddings.append(all_code_embeddings[correct_code_indices[i]]) # プール先頭に正解コードの埋め込みを追加\n",
        "\n",
        "        # 不正解コードを候補プールに追加 (データセット全体からランダムサンプリング)\n",
        "        incorrect_code_indices = []\n",
        "        while len(incorrect_code_indices) < pool_size - 1:\n",
        "            rand_index = random.randint(0, num_examples - 1) # 不正解コードのインデックスをランダムに選択 (max_examplesまで)\n",
        "            if rand_index != correct_code_indices[i] and rand_index not in incorrect_code_indices: # 正解コードと重複しないようにチェック\n",
        "                incorrect_code_indices.append(rand_index)\n",
        "        for incorrect_index in incorrect_code_indices:\n",
        "            candidate_pool_embeddings.append(all_code_embeddings[incorrect_index])\n",
        "        candidate_pool_embeddings = np.stack(candidate_pool_embeddings, axis=0) # shape: (pool_size, hidden_size) # ★np.stack に変更\n",
        "\n",
        "\n",
        "\n",
        "        # クエリと候補コードプール間のコサイン類似度を計算\n",
        "        sims = cosine_similarity(query_embedding, candidate_pool_embeddings)[0] # shape: (pool_size,)\n",
        "        sim_matrices.append(sims) # shape: (num_queries, pool_size)\n",
        "    sim_matrix = np.array(sim_matrices) # shape: (num_queries, pool_size)\n",
        "    print(\"類似度行列計算完了.\")\n",
        "\n",
        "\n",
        "    # 評価指標の計算\n",
        "    metrics = calculate_metrics(sim_matrix)\n",
        "    return metrics\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # CodeSearchNet の Python 部分のテストセットをロード\n",
        "    print(\"CodeSearchNet データセットをロードします...\")\n",
        "    dataset = load_dataset(\"code_search_net\", \"python\", split=\"test\", trust_remote_code=True)\n",
        "    # デモ用に上位 1000 件を評価対象にする\n",
        "    subset = dataset.select(range(1000)) # 評価サンプル数を調整\n",
        "\n",
        "    ### CodeMorph-BERT の評価 ###\n",
        "    model_name1 = \"Shuu12121/CodeMorph-BERT\"\n",
        "    print(f\"\\n{model_name1} を評価します (候補プールサイズ: 100)...\")\n",
        "    tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n",
        "    model1 = AutoModelForMaskedLM.from_pretrained(model_name1) # AutoModelForMaskedLM を使用\n",
        "    model1.to(device)\n",
        "    metrics1 = evaluate_code_search(model1, tokenizer1, subset, device, max_examples=1000, pool_size=100) # max_examples, pool_size を調整\n",
        "    print(f\"CodeMorph-BERT の評価指標 (候補プールサイズ: 100):\")\n",
        "    for name, value_dict in metrics1.items(): # metrics1 は辞書の中に辞書を持つ構造 (metrics1.items() で外側の辞書をiterate)\n",
        "        if isinstance(value_dict, dict): # Recall@k, Precision@k, NDCG@k の場合 (辞書型)\n",
        "            for k, v in value_dict.items(): # value_dict.items() で内側の辞書をiterate\n",
        "                print(f\"  {name}@{k}: {v:.4f}\")\n",
        "        else: # MRR の場合 (辞書型ではない)\n",
        "            print(f\"  {name}: {value_dict:.4f}\")\n",
        "\n",
        "\n",
        "    ### Microsoft CodeBERT (microsoft/codebert-base-mlm) の評価 ###\n",
        "    model_name2 = \"microsoft/codebert-base-mlm\"\n",
        "    print(f\"\\n{model_name2} を評価します (候補プールサイズ: 100)...\")\n",
        "    tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
        "    model2 = AutoModelForMaskedLM.from_pretrained(model_name2) # AutoModelForMaskedLM を使用\n",
        "    model2.to(device)\n",
        "    metrics2 = evaluate_code_search(model2, tokenizer2, subset, device, max_examples=1000, pool_size=100) # max_examples, pool_size を調整\n",
        "    print(f\"CodeBERT (microsoft/codebert-base-mlm) の評価指標 (候補プールサイズ: 100):\")\n",
        "    for name, value_dict in metrics2.items(): # metrics2 は辞書の中に辞書を持つ構造 (metrics2.items() で外側の辞書をiterate)\n",
        "        if isinstance(value_dict, dict): # Recall@k, Precision@k, NDCG@k の場合 (辞書型)\n",
        "            for k, v in value_dict.items(): # value_dict.items() で内側の辞書をiterate\n",
        "                print(f\"  {name}@{k}: {v:.4f}\")\n",
        "        else: # MRR の場合 (辞書型ではない)\n",
        "            print(f\"  {name}: {value_dict:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oawz5-085ybw",
        "outputId": "b60a17cf-562c-4bb0-f89e-98695fde7171"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用デバイス: cuda\n",
            "CodeSearchNet データセットをロードします...\n",
            "\n",
            "Shuu12121/CodeMorph-BERT を評価します (候補プールサイズ: 100)...\n",
            "データセット全体のコード埋め込みを計算...\n",
            "コード埋め込み計算完了.\n",
            "候補コードプールを作成し、類似度行列を計算...\n",
            "類似度行列計算完了.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/codebert-base-mlm were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CodeMorph-BERT の評価指標 (候補プールサイズ: 100):\n",
            "  mrr: 0.8929\n",
            "  recall@k@1: 0.8520\n",
            "  recall@k@5: 0.9460\n",
            "  recall@k@10: 0.9660\n",
            "  recall@k@50: 0.9910\n",
            "  recall@k@100: 1.0000\n",
            "  precision@k@1: 0.8520\n",
            "  precision@k@5: 0.8885\n",
            "  precision@k@10: 0.8912\n",
            "  precision@k@50: 0.8927\n",
            "  precision@k@100: 0.8929\n",
            "  ndcg@k@1: 0.8520\n",
            "  ndcg@k@5: 0.9029\n",
            "  ndcg@k@10: 0.9094\n",
            "  ndcg@k@50: 0.9155\n",
            "  ndcg@k@100: 0.9170\n",
            "  map: 0.8929\n",
            "  f1@k@1: 0.8520\n",
            "  f1@k@5: 0.9036\n",
            "  f1@k@10: 0.9084\n",
            "  f1@k@50: 0.9113\n",
            "  f1@k@100: 0.9115\n",
            "  r_precision: 0.8520\n",
            "  success_rate@k@1: 0.8520\n",
            "  success_rate@k@5: 0.9460\n",
            "  success_rate@k@10: 0.9660\n",
            "  success_rate@k@50: 0.9910\n",
            "  success_rate@k@100: 1.0000\n",
            "  query_coverage@k@1: 0.8520\n",
            "  query_coverage@k@5: 0.9460\n",
            "  query_coverage@k@10: 0.9660\n",
            "  query_coverage@k@50: 0.9910\n",
            "  query_coverage@k@100: 1.0000\n",
            "\n",
            "microsoft/codebert-base-mlm を評価します (候補プールサイズ: 100)...\n",
            "データセット全体のコード埋め込みを計算...\n",
            "コード埋め込み計算完了.\n",
            "候補コードプールを作成し、類似度行列を計算...\n",
            "類似度行列計算完了.\n",
            "CodeBERT (microsoft/codebert-base-mlm) の評価指標 (候補プールサイズ: 100):\n",
            "  mrr: 0.8064\n",
            "  recall@k@1: 0.7560\n",
            "  recall@k@5: 0.8590\n",
            "  recall@k@10: 0.9050\n",
            "  recall@k@50: 0.9880\n",
            "  recall@k@100: 1.0000\n",
            "  precision@k@1: 0.7560\n",
            "  precision@k@5: 0.7956\n",
            "  precision@k@10: 0.8015\n",
            "  precision@k@50: 0.8062\n",
            "  precision@k@100: 0.8064\n",
            "  ndcg@k@1: 0.7560\n",
            "  ndcg@k@5: 0.8114\n",
            "  ndcg@k@10: 0.8261\n",
            "  ndcg@k@50: 0.8455\n",
            "  ndcg@k@100: 0.8475\n",
            "  map: 0.8064\n",
            "  f1@k@1: 0.7560\n",
            "  f1@k@5: 0.8119\n",
            "  f1@k@10: 0.8224\n",
            "  f1@k@50: 0.8312\n",
            "  f1@k@100: 0.8316\n",
            "  r_precision: 0.7560\n",
            "  success_rate@k@1: 0.7560\n",
            "  success_rate@k@5: 0.8590\n",
            "  success_rate@k@10: 0.9050\n",
            "  success_rate@k@50: 0.9880\n",
            "  success_rate@k@100: 1.0000\n",
            "  query_coverage@k@1: 0.7560\n",
            "  query_coverage@k@5: 0.8590\n",
            "  query_coverage@k@10: 0.9050\n",
            "  query_coverage@k@50: 0.9880\n",
            "  query_coverage@k@100: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer, RobertaForMaskedLM, BertForMaskedLM\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "\n",
        "# デバイス設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用デバイス: {device}\")\n",
        "\n",
        "def get_cls_embedding(model, tokenizer, text, device, max_length=256):\n",
        "    \"\"\"\n",
        "    入力テキストから [CLS] トークンの埋め込みを取得する関数。\n",
        "    ※モデルが BERT 系の場合は model.bert を、RoBERTa 系の場合は model.roberta を利用して出力を取得します。\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    if isinstance(model, BertForMaskedLM):\n",
        "        outputs = model.bert(**inputs)\n",
        "    elif isinstance(model, RobertaForMaskedLM):\n",
        "        outputs = model.roberta(**inputs)\n",
        "    else:\n",
        "        raise ValueError(\"Model must be BertForMaskedLM or RobertaForMaskedLM.\")\n",
        "\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "    return cls_embedding.detach().cpu().numpy()\n",
        "\n",
        "def calculate_metrics(sim_matrix, k_values=[1, 5, 10, 50, 100]):\n",
        "    num_queries = sim_matrix.shape[0]\n",
        "    metrics = {\n",
        "        \"mrr\": 0.0,\n",
        "        \"recall@k\": {k: 0.0 for k in k_values},\n",
        "        \"precision@k\": {k: 0.0 for k in k_values},\n",
        "        \"ndcg@k\": {k: 0.0 for k in k_values},\n",
        "        \"map\": 0.0,\n",
        "        \"f1@k\": {k: 0.0 for k in k_values},\n",
        "        \"r_precision\": 0.0,\n",
        "        \"success_rate@k\": {k: 0.0 for k in k_values},\n",
        "        \"query_coverage@k\": {k: 0.0 for k in k_values},\n",
        "    }\n",
        "\n",
        "    for i in range(num_queries):\n",
        "        sims = sim_matrix[i]\n",
        "        ranked_indices = np.argsort(-sims)\n",
        "        correct_rank = np.where(ranked_indices == 0)[0][0] + 1\n",
        "        is_correct_in_top_k = {k: correct_rank <= k for k in k_values}\n",
        "\n",
        "        # MRR\n",
        "        metrics[\"mrr\"] += 1.0 / correct_rank\n",
        "\n",
        "        # 各 k に対してクエリごとの precision, recall, f1 を計算\n",
        "        for k in k_values:\n",
        "            if correct_rank <= k:\n",
        "                # 正解がトップkに含まれる場合\n",
        "                prec = 1.0 / correct_rank\n",
        "                rec = 1.0  # 候補が1件の場合、正解が含まれていれば recall は 1.0\n",
        "            else:\n",
        "                prec = 0.0\n",
        "                rec = 0.0\n",
        "            f1 = calculate_f1(prec, rec)\n",
        "\n",
        "            metrics[\"precision@k\"][k] += prec\n",
        "            metrics[\"recall@k\"][k] += rec\n",
        "            metrics[\"f1@k\"][k] += f1\n",
        "\n",
        "            # NDCG\n",
        "            ideal_ranking = [1.0] + [0.0] * (k - 1)\n",
        "            actual_ranking = [1.0 if j == 0 else 0.0 for j in ranked_indices[:k]]\n",
        "            idcg = calculate_dcg(ideal_ranking)\n",
        "            dcg = calculate_dcg(actual_ranking)\n",
        "            metrics[\"ndcg@k\"][k] += dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "            # Success Rate, Query Coverage\n",
        "            metrics[\"success_rate@k\"][k] += 1.0 if is_correct_in_top_k[k] else 0.0\n",
        "            metrics[\"query_coverage@k\"][k] += 1.0 if is_correct_in_top_k[k] else 0.0\n",
        "\n",
        "        # MAP, R-Precision\n",
        "        metrics[\"map\"] += calculate_average_precision(ranked_indices)\n",
        "        metrics[\"r_precision\"] += calculate_r_precision(ranked_indices)\n",
        "\n",
        "    # 各指標をクエリ数で平均化\n",
        "    metrics[\"mrr\"] /= num_queries\n",
        "    metrics[\"map\"] /= num_queries\n",
        "    metrics[\"r_precision\"] /= num_queries\n",
        "    for k in k_values:\n",
        "        metrics[\"recall@k\"][k] /= num_queries\n",
        "        metrics[\"precision@k\"][k] /= num_queries\n",
        "        metrics[\"ndcg@k\"][k] /= num_queries\n",
        "        metrics[\"f1@k\"][k] /= num_queries\n",
        "        metrics[\"success_rate@k\"][k] /= num_queries\n",
        "        metrics[\"query_coverage@k\"][k] /= num_queries\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def calculate_average_precision(ranked_indices):\n",
        "    correct_rank = np.where(ranked_indices == 0)[0][0] + 1\n",
        "    ap = 0.0\n",
        "    for k in range(1, len(ranked_indices) + 1):\n",
        "        if k == correct_rank:\n",
        "            ap += 1.0 / k\n",
        "    return ap\n",
        "\n",
        "def calculate_f1(precision, recall):\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "    return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "def calculate_r_precision(ranked_indices):\n",
        "    r = 1\n",
        "    correct_in_top_r = 0\n",
        "    for i in range(r):\n",
        "        if ranked_indices[i] == 0:\n",
        "            correct_in_top_r += 1\n",
        "    return correct_in_top_r / r\n",
        "\n",
        "def calculate_dcg(ranking):\n",
        "    dcg = 0.0\n",
        "    for i, rel in enumerate(ranking):\n",
        "        dcg += rel / np.log2(i + 2)\n",
        "    return dcg\n",
        "\n",
        "def evaluate_code_search(model, tokenizer, dataset, device, max_examples=100, pool_size=100,\n",
        "                         query_field=\"func_documentation_string\", code_field=\"func_code_string\"):\n",
        "    \"\"\"\n",
        "    クエリとコードのペアから、候補プールを作成し各種評価指標を算出する関数です。\n",
        "    フィールド名は dataset 内の実際のカラム名に合わせて指定してください。\n",
        "    \"\"\"\n",
        "    num_examples = min(len(dataset), max_examples)\n",
        "    all_codes = [dataset[i][code_field] for i in range(len(dataset))]\n",
        "    queries = []\n",
        "    correct_code_indices = []\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        ex = dataset[i]\n",
        "        query_text = ex[query_field]\n",
        "        queries.append(query_text)\n",
        "        correct_code_indices.append(i)\n",
        "\n",
        "    # コードの埋め込みを事前計算\n",
        "    all_code_embeddings = []\n",
        "    print(\"データセット全体のコード埋め込みを計算中...\")\n",
        "    for code in all_codes[:num_examples]:\n",
        "        emb = get_cls_embedding(model, tokenizer, code, device)\n",
        "        all_code_embeddings.append(emb)\n",
        "    all_code_embeddings = np.concatenate(all_code_embeddings, axis=0)\n",
        "    print(\"コード埋め込み計算完了.\")\n",
        "\n",
        "    sim_matrices = []\n",
        "    print(\"候補コードプールを作成し、類似度計算中...\")\n",
        "    for i in range(num_examples):\n",
        "        query_embedding = get_cls_embedding(model, tokenizer, queries[i], device).reshape(1, -1)\n",
        "        candidate_pool_embeddings = []\n",
        "        candidate_pool_embeddings.append(all_code_embeddings[correct_code_indices[i]])\n",
        "        incorrect_code_indices = []\n",
        "        while len(incorrect_code_indices) < pool_size - 1:\n",
        "            rand_index = random.randint(0, num_examples - 1)\n",
        "            if rand_index != correct_code_indices[i] and rand_index not in incorrect_code_indices:\n",
        "                incorrect_code_indices.append(rand_index)\n",
        "        for incorrect_index in incorrect_code_indices:\n",
        "            candidate_pool_embeddings.append(all_code_embeddings[incorrect_index])\n",
        "        candidate_pool_embeddings = np.stack(candidate_pool_embeddings, axis=0)\n",
        "\n",
        "\n",
        "        sims = cosine_similarity(query_embedding, candidate_pool_embeddings)[0]\n",
        "        sim_matrices.append(sims)\n",
        "    sim_matrix = np.array(sim_matrices)\n",
        "    print(\"類似度計算完了.\")\n",
        "\n",
        "    metrics = calculate_metrics(sim_matrix)\n",
        "    return metrics\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    ######################################\n",
        "    # ② google/code_x_glue_tc_nl_code_search_adv を用いた評価例\n",
        "    ######################################\n",
        "    print(\"\\ngoogle/code_x_glue_tc_nl_code_search_adv データセット (Test) をロードします...\")\n",
        "    tc_dataset = load_dataset(\"google/code_x_glue_tc_nl_code_search_adv\", split=\"test\")\n",
        "    subset_tc = tc_dataset.select(range(10000))  # 必要に応じてサンプル数を調整\n",
        "\n",
        "    # ここでは、クエリとして \"docstring\"、コードとして \"code\" のカラムを利用\n",
        "    model_name1 = \"Shuu12121/CodeMorph-BERT\"\n",
        "    print(f\"\\n{model_name1} を google/code_x_glue_tc_nl_code_search_adv で評価します (候補プールサイズ: 100)...\")\n",
        "    tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n",
        "    model1 = AutoModelForMaskedLM.from_pretrained(model_name1)\n",
        "    model1.to(device)\n",
        "    metrics_tc1 = evaluate_code_search(model1, tokenizer1, subset_tc, device, max_examples=10000, pool_size=100,\n",
        "                                       query_field=\"docstring\", code_field=\"code\")\n",
        "    print(f\"{model_name1} の評価指標 (google/code_x_glue_tc_nl_code_search_adv):\")\n",
        "    for name, value in metrics_tc1.items():\n",
        "        if isinstance(value, dict):\n",
        "            for k, v in value.items():\n",
        "                print(f\"  {name}@{k}: {v:.4f}\")\n",
        "        else:\n",
        "            print(f\"  {name}: {value:.4f}\")\n",
        "\n",
        "    # 例として、Microsoft CodeBERT でも評価する場合\n",
        "    model_name2 = \"microsoft/codebert-base-mlm\"\n",
        "    print(f\"\\n{model_name2} を google/code_x_glue_tc_nl_code_search_adv で評価します (候補プールサイズ: 100)...\")\n",
        "    tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
        "    model2 = AutoModelForMaskedLM.from_pretrained(model_name2)\n",
        "    model2.to(device)\n",
        "    metrics_tc2 = evaluate_code_search(model2, tokenizer2, subset_tc, device, max_examples=10000, pool_size=100,\n",
        "                                       query_field=\"docstring\", code_field=\"code\")\n",
        "    print(f\"{model_name2} の評価指標 (google/code_x_glue_tc_nl_code_search_adv):\")\n",
        "    for name, value in metrics_tc2.items():\n",
        "        if isinstance(value, dict):\n",
        "            for k, v in value.items():\n",
        "                print(f\"  {name}@{k}: {v:.4f}\")\n",
        "        else:\n",
        "            print(f\"  {name}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwsga6SE_Mnj",
        "outputId": "b77b8f71-3509-4e10-80c6-f6e8562430cb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用デバイス: cuda\n",
            "\n",
            "google/code_x_glue_tc_nl_code_search_adv データセット (Test) をロードします...\n",
            "\n",
            "Shuu12121/CodeMorph-BERT を google/code_x_glue_tc_nl_code_search_adv で評価します (候補プールサイズ: 100)...\n",
            "データセット全体のコード埋め込みを計算中...\n",
            "コード埋め込み計算完了.\n",
            "候補コードプールを作成し、類似度計算中...\n",
            "類似度計算完了.\n",
            "Shuu12121/CodeMorph-BERT の評価指標 (google/code_x_glue_tc_nl_code_search_adv):\n",
            "  mrr: 0.7788\n",
            "  recall@k@1: 0.7159\n",
            "  recall@k@5: 0.8509\n",
            "  recall@k@10: 0.8990\n",
            "  recall@k@50: 0.9847\n",
            "  recall@k@100: 1.0000\n",
            "  precision@k@1: 0.7159\n",
            "  precision@k@5: 0.7676\n",
            "  precision@k@10: 0.7741\n",
            "  precision@k@50: 0.7786\n",
            "  precision@k@100: 0.7788\n",
            "  ndcg@k@1: 0.7159\n",
            "  ndcg@k@5: 0.7885\n",
            "  ndcg@k@10: 0.8040\n",
            "  ndcg@k@50: 0.8236\n",
            "  ndcg@k@100: 0.8262\n",
            "  map: 0.7788\n",
            "  f1@k@1: 0.7159\n",
            "  f1@k@5: 0.7892\n",
            "  f1@k@10: 0.8005\n",
            "  f1@k@50: 0.8090\n",
            "  f1@k@100: 0.8095\n",
            "  r_precision: 0.7159\n",
            "  success_rate@k@1: 0.7159\n",
            "  success_rate@k@5: 0.8509\n",
            "  success_rate@k@10: 0.8990\n",
            "  success_rate@k@50: 0.9847\n",
            "  success_rate@k@100: 1.0000\n",
            "  query_coverage@k@1: 0.7159\n",
            "  query_coverage@k@5: 0.8509\n",
            "  query_coverage@k@10: 0.8990\n",
            "  query_coverage@k@50: 0.9847\n",
            "  query_coverage@k@100: 1.0000\n",
            "\n",
            "microsoft/codebert-base-mlm を google/code_x_glue_tc_nl_code_search_adv で評価します (候補プールサイズ: 100)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/codebert-base-mlm were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "データセット全体のコード埋め込みを計算中...\n",
            "コード埋め込み計算完了.\n",
            "候補コードプールを作成し、類似度計算中...\n",
            "類似度計算完了.\n",
            "microsoft/codebert-base-mlm の評価指標 (google/code_x_glue_tc_nl_code_search_adv):\n",
            "  mrr: 0.7703\n",
            "  recall@k@1: 0.7205\n",
            "  recall@k@5: 0.8193\n",
            "  recall@k@10: 0.8781\n",
            "  recall@k@50: 0.9805\n",
            "  recall@k@100: 1.0000\n",
            "  precision@k@1: 0.7205\n",
            "  precision@k@5: 0.7565\n",
            "  precision@k@10: 0.7644\n",
            "  precision@k@50: 0.7700\n",
            "  precision@k@100: 0.7703\n",
            "  ndcg@k@1: 0.7205\n",
            "  ndcg@k@5: 0.7721\n",
            "  ndcg@k@10: 0.7912\n",
            "  ndcg@k@50: 0.8149\n",
            "  ndcg@k@100: 0.8180\n",
            "  map: 0.7703\n",
            "  f1@k@1: 0.7205\n",
            "  f1@k@5: 0.7722\n",
            "  f1@k@10: 0.7861\n",
            "  f1@k@50: 0.7965\n",
            "  f1@k@100: 0.7971\n",
            "  r_precision: 0.7205\n",
            "  success_rate@k@1: 0.7205\n",
            "  success_rate@k@5: 0.8193\n",
            "  success_rate@k@10: 0.8781\n",
            "  success_rate@k@50: 0.9805\n",
            "  success_rate@k@100: 1.0000\n",
            "  query_coverage@k@1: 0.7205\n",
            "  query_coverage@k@5: 0.8193\n",
            "  query_coverage@k@10: 0.8781\n",
            "  query_coverage@k@50: 0.9805\n",
            "  query_coverage@k@100: 1.0000\n"
          ]
        }
      ]
    }
  ]
}