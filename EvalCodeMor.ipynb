{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMC3OgEJnIKll5950VYzYdf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shun0212/CodeBERTPretrained/blob/main/EvalCodeMor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH5_cdm9-J0Y",
        "outputId": "3ab94fd9-463c-4c11-fc14-54fa568fec48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer, RobertaForMaskedLM, BertForMaskedLM\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "\n",
        "# デバイス設定\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用デバイス: {device}\")\n",
        "\n",
        "def get_cls_embedding(model, tokenizer, text, device, max_length=256):\n",
        "    \"\"\"\n",
        "    入力テキストから [CLS] トークンの埋め込みを取得する関数。\n",
        "    ※モデルが BERT 系の場合は model.bert を、RoBERTa 系の場合は model.roberta を利用して出力を取得します。\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # モデルの種類に応じて内部のエンコーダにアクセスする\n",
        "    if isinstance(model, BertForMaskedLM): # BERT 系モデルの判定を修正\n",
        "        outputs = model.bert(**inputs)\n",
        "    elif isinstance(model, RobertaForMaskedLM): # RoBERTa 系モデルの判定を修正\n",
        "        outputs = model.roberta(**inputs)\n",
        "    else:\n",
        "        raise ValueError(\"Model must be BertForMaskedLM or RobertaForMaskedLM.\") # エラーメッセージを修正\n",
        "\n",
        "    # outputs.last_hidden_state: (batch_size, sequence_length, hidden_size)\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] または最初のトークンの埋め込み\n",
        "    return cls_embedding.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def calculate_metrics(sim_matrix, k_values=[1, 5, 10, 50, 100]):\n",
        "    num_queries = sim_matrix.shape[0]\n",
        "    metrics = {\n",
        "        \"mrr\": 0.0,\n",
        "        \"recall@k\": {k: 0.0 for k in k_values},\n",
        "        \"precision@k\": {k: 0.0 for k in k_values},\n",
        "        \"ndcg@k\": {k: 0.0 for k in k_values},\n",
        "        \"map\": 0.0,\n",
        "        \"f1@k\": {k: 0.0 for k in k_values},\n",
        "        \"r_precision\": 0.0,\n",
        "        \"success_rate@k\": {k: 0.0 for k in k_values},\n",
        "        \"query_coverage@k\": {k: 0.0 for k in k_values},\n",
        "    }\n",
        "\n",
        "    for i in range(num_queries):\n",
        "        sims = sim_matrix[i]\n",
        "        ranked_indices = np.argsort(-sims)\n",
        "        correct_rank = np.where(ranked_indices == 0)[0][0] + 1\n",
        "        is_correct_in_top_k = {k: correct_rank <= k for k in k_values}\n",
        "\n",
        "        # MRR\n",
        "        metrics[\"mrr\"] += 1.0 / correct_rank\n",
        "\n",
        "        # 各 k に対してクエリごとの precision, recall, f1 を計算\n",
        "        for k in k_values:\n",
        "            if correct_rank <= k:\n",
        "                # 正解がトップkに含まれる場合\n",
        "                prec = 1.0 / correct_rank\n",
        "                rec = 1.0  # 候補が1件の場合、正解が含まれていれば recall は 1.0\n",
        "            else:\n",
        "                prec = 0.0\n",
        "                rec = 0.0\n",
        "            f1 = calculate_f1(prec, rec)\n",
        "\n",
        "            metrics[\"precision@k\"][k] += prec\n",
        "            metrics[\"recall@k\"][k] += rec\n",
        "            metrics[\"f1@k\"][k] += f1\n",
        "\n",
        "            # NDCG\n",
        "            ideal_ranking = [1.0] + [0.0] * (k - 1)\n",
        "            actual_ranking = [1.0 if j == 0 else 0.0 for j in ranked_indices[:k]]\n",
        "            idcg = calculate_dcg(ideal_ranking)\n",
        "            dcg = calculate_dcg(actual_ranking)\n",
        "            metrics[\"ndcg@k\"][k] += dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "            # Success Rate, Query Coverage\n",
        "            metrics[\"success_rate@k\"][k] += 1.0 if is_correct_in_top_k[k] else 0.0\n",
        "            metrics[\"query_coverage@k\"][k] += 1.0 if is_correct_in_top_k[k] else 0.0\n",
        "\n",
        "        # MAP, R-Precision\n",
        "        metrics[\"map\"] += calculate_average_precision(ranked_indices)\n",
        "        metrics[\"r_precision\"] += calculate_r_precision(ranked_indices)\n",
        "\n",
        "    # 各指標をクエリ数で平均化\n",
        "    metrics[\"mrr\"] /= num_queries\n",
        "    metrics[\"map\"] /= num_queries\n",
        "    metrics[\"r_precision\"] /= num_queries\n",
        "    for k in k_values:\n",
        "        metrics[\"recall@k\"][k] /= num_queries\n",
        "        metrics[\"precision@k\"][k] /= num_queries\n",
        "        metrics[\"ndcg@k\"][k] /= num_queries\n",
        "        metrics[\"f1@k\"][k] /= num_queries\n",
        "        metrics[\"success_rate@k\"][k] /= num_queries\n",
        "        metrics[\"query_coverage@k\"][k] /= num_queries\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def calculate_average_precision(ranked_indices): # ★ MAP 計算用関数を追加\n",
        "    \"\"\"\n",
        "    Average Precision (AP) を計算する関数。\n",
        "    \"\"\"\n",
        "    correct_rank = np.where(ranked_indices == 0)[0][0] + 1\n",
        "    ap = 0.0\n",
        "    for k in range(1, len(ranked_indices) + 1):\n",
        "        if k == correct_rank: # k番目に正解コードが現れた場合のみ Precision を加算\n",
        "            ap += 1.0 / k\n",
        "    return ap\n",
        "\n",
        "\n",
        "def calculate_f1(precision, recall): # ★ F1値計算用関数を追加\n",
        "    \"\"\"\n",
        "    F1値を計算する関数 (Precision と Recall から計算)。\n",
        "    \"\"\"\n",
        "    if precision + recall == 0: # Precision, Recall が共に 0 の場合\n",
        "        return 0.0\n",
        "    return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "\n",
        "def calculate_r_precision(ranked_indices): # ★ R-Precision 計算用関数を追加\n",
        "    \"\"\"\n",
        "    R-Precision を計算する関数 (R=1 で固定: 候補プールに正解コードは1つのみ)。\n",
        "    \"\"\"\n",
        "    r = 1 # 正解コード数 (候補プールに1つのみ)\n",
        "    correct_in_top_r = 0\n",
        "    for i in range(r): # 上位 r 件まで確認\n",
        "        if ranked_indices[i] == 0: # 正解コードがランクイン\n",
        "            correct_in_top_r += 1\n",
        "    return correct_in_top_r / r # R-Precision は適合率\n",
        "\n",
        "def calculate_dcg(ranking):\n",
        "    \"\"\"\n",
        "    Discounted Cumulative Gain (DCG) を計算する関数。\n",
        "    \"\"\"\n",
        "    dcg = 0.0\n",
        "    for i, rel in enumerate(ranking):\n",
        "        dcg += rel / np.log2(i + 2) # 順位 i+1 の割引率: 1/log2(i+2)\n",
        "    return dcg\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "def remove_multiline_strings(code):\n",
        "    \"\"\"\n",
        "    Pythonコードからマルチライン文字列（\"\"\" \"\"\" や ''' '''）を削除する関数\n",
        "    \"\"\"\n",
        "    pattern = r'(\"\"\"(.*?)\"\"\"|\\'\\'\\'(.*?)\\'\\'\\')'\n",
        "    return re.sub(pattern, '', code, flags=re.DOTALL)\n",
        "\n",
        "def evaluate_code_search(model, tokenizer, dataset, device, max_examples=100, pool_size=100,\n",
        "                         query_field=\"func_documentation_string\", code_field=\"func_code_string\"):\n",
        "    num_examples = min(len(dataset), max_examples)\n",
        "    all_codes = [remove_multiline_strings(dataset[i][code_field]) for i in range(len(dataset))]\n",
        "    queries = []\n",
        "    correct_code_indices = []\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        ex = dataset[i]\n",
        "        query_text = ex[query_field]\n",
        "        queries.append(query_text)\n",
        "        correct_code_indices.append(i)\n",
        "\n",
        "    all_code_embeddings = []\n",
        "    print(\"データセット全体のコード埋め込みを計算中...\")\n",
        "    for code in all_codes[:num_examples]:\n",
        "        emb = get_cls_embedding(model, tokenizer, code, device)\n",
        "        all_code_embeddings.append(emb)\n",
        "    all_code_embeddings = np.concatenate(all_code_embeddings, axis=0)\n",
        "    print(\"コード埋め込み計算完了.\")\n",
        "\n",
        "    sim_matrices = []\n",
        "    print(\"候補コードプールを作成し、類似度計算中...\")\n",
        "    for i in range(num_examples):\n",
        "        query_embedding = get_cls_embedding(model, tokenizer, queries[i], device).reshape(1, -1)\n",
        "        candidate_pool_embeddings = []\n",
        "        candidate_pool_embeddings.append(all_code_embeddings[correct_code_indices[i]])\n",
        "        incorrect_code_indices = []\n",
        "        while len(incorrect_code_indices) < pool_size - 1:\n",
        "            rand_index = random.randint(0, num_examples - 1)\n",
        "            if rand_index != correct_code_indices[i] and rand_index not in incorrect_code_indices:\n",
        "                incorrect_code_indices.append(rand_index)\n",
        "        for incorrect_index in incorrect_code_indices:\n",
        "            candidate_pool_embeddings.append(all_code_embeddings[incorrect_index])\n",
        "        candidate_pool_embeddings = np.stack(candidate_pool_embeddings, axis=0)\n",
        "        sims = cosine_similarity(query_embedding, candidate_pool_embeddings)[0]\n",
        "        sim_matrices.append(sims)\n",
        "    sim_matrix = np.array(sim_matrices)\n",
        "    print(\"類似度計算完了.\")\n",
        "    metrics = calculate_metrics(sim_matrix)\n",
        "    return metrics\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # CodeSearchNet の Python 部分のテストセットをロード\n",
        "    print(\"CodeSearchNet データセットをロードします...\")\n",
        "    dataset = load_dataset(\"code_search_net\", \"python\", split=\"test\", trust_remote_code=True)\n",
        "    # デモ用に上位 1000 件を評価対象にする\n",
        "    subset = dataset.select(range(1000)) # 評価サンプル数を調整\n",
        "\n",
        "    ### CodeMorph-BERT の評価 ###\n",
        "    model_name1 = \"Shuu12121/CodeMorph-BERT\"\n",
        "    print(f\"\\n{model_name1} を評価します (候補プールサイズ: 100)...\")\n",
        "    tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n",
        "    model1 = AutoModelForMaskedLM.from_pretrained(model_name1) # AutoModelForMaskedLM を使用\n",
        "    model1.to(device)\n",
        "    metrics1 = evaluate_code_search(model1, tokenizer1, subset, device, max_examples=1000, pool_size=100) # max_examples, pool_size を調整\n",
        "    print(f\"CodeMorph-BERT の評価指標 (候補プールサイズ: 100):\")\n",
        "    for name, value_dict in metrics1.items(): # metrics1 は辞書の中に辞書を持つ構造 (metrics1.items() で外側の辞書をiterate)\n",
        "        if isinstance(value_dict, dict): # Recall@k, Precision@k, NDCG@k の場合 (辞書型)\n",
        "            for k, v in value_dict.items(): # value_dict.items() で内側の辞書をiterate\n",
        "                print(f\"  {name}@{k}: {v:.4f}\")\n",
        "        else: # MRR の場合 (辞書型ではない)\n",
        "            print(f\"  {name}: {value_dict:.4f}\")\n",
        "\n",
        "\n",
        "    ### Microsoft CodeBERT (microsoft/codebert-base-mlm) の評価 ###\n",
        "    model_name2 = \"microsoft/codebert-base-mlm\"\n",
        "    print(f\"\\n{model_name2} を評価します (候補プールサイズ: 100)...\")\n",
        "    tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
        "    model2 = AutoModelForMaskedLM.from_pretrained(model_name2) # AutoModelForMaskedLM を使用\n",
        "    model2.to(device)\n",
        "    metrics2 = evaluate_code_search(model2, tokenizer2, subset, device, max_examples=1000, pool_size=100) # max_examples, pool_size を調整\n",
        "    print(f\"CodeBERT (microsoft/codebert-base-mlm) の評価指標 (候補プールサイズ: 100):\")\n",
        "    for name, value_dict in metrics2.items(): # metrics2 は辞書の中に辞書を持つ構造 (metrics2.items() で外側の辞書をiterate)\n",
        "        if isinstance(value_dict, dict): # Recall@k, Precision@k, NDCG@k の場合 (辞書型)\n",
        "            for k, v in value_dict.items(): # value_dict.items() で内側の辞書をiterate\n",
        "                print(f\"  {name}@{k}: {v:.4f}\")\n",
        "        else: # MRR の場合 (辞書型ではない)\n",
        "            print(f\"  {name}: {value_dict:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oawz5-085ybw",
        "outputId": "87237717-9030-4c87-8068-87acd7300f24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用デバイス: cuda\n",
            "CodeSearchNet データセットをロードします...\n",
            "\n",
            "Shuu12121/CodeMorph-BERT を評価します (候補プールサイズ: 100)...\n",
            "データセット全体のコード埋め込みを計算中...\n",
            "コード埋め込み計算完了.\n",
            "候補コードプールを作成し、類似度計算中...\n",
            "類似度計算完了.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/codebert-base-mlm were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CodeMorph-BERT の評価指標 (候補プールサイズ: 100):\n",
            "  mrr: 0.6678\n",
            "  recall@k@1: 0.5650\n",
            "  recall@k@5: 0.7970\n",
            "  recall@k@10: 0.8600\n",
            "  recall@k@50: 0.9770\n",
            "  recall@k@100: 1.0000\n",
            "  precision@k@1: 0.5650\n",
            "  precision@k@5: 0.6526\n",
            "  precision@k@10: 0.6610\n",
            "  precision@k@50: 0.6675\n",
            "  precision@k@100: 0.6678\n",
            "  ndcg@k@1: 0.5650\n",
            "  ndcg@k@5: 0.6887\n",
            "  ndcg@k@10: 0.7091\n",
            "  ndcg@k@50: 0.7363\n",
            "  ndcg@k@100: 0.7401\n",
            "  map: 0.6678\n",
            "  f1@k@1: 0.5650\n",
            "  f1@k@5: 0.6899\n",
            "  f1@k@10: 0.7046\n",
            "  f1@k@50: 0.7169\n",
            "  f1@k@100: 0.7175\n",
            "  r_precision: 0.5650\n",
            "  success_rate@k@1: 0.5650\n",
            "  success_rate@k@5: 0.7970\n",
            "  success_rate@k@10: 0.8600\n",
            "  success_rate@k@50: 0.9770\n",
            "  success_rate@k@100: 1.0000\n",
            "  query_coverage@k@1: 0.5650\n",
            "  query_coverage@k@5: 0.7970\n",
            "  query_coverage@k@10: 0.8600\n",
            "  query_coverage@k@50: 0.9770\n",
            "  query_coverage@k@100: 1.0000\n",
            "\n",
            "microsoft/codebert-base-mlm を評価します (候補プールサイズ: 100)...\n",
            "データセット全体のコード埋め込みを計算中...\n",
            "コード埋め込み計算完了.\n",
            "候補コードプールを作成し、類似度計算中...\n",
            "類似度計算完了.\n",
            "CodeBERT (microsoft/codebert-base-mlm) の評価指標 (候補プールサイズ: 100):\n",
            "  mrr: 0.5598\n",
            "  recall@k@1: 0.4650\n",
            "  recall@k@5: 0.6490\n",
            "  recall@k@10: 0.7410\n",
            "  recall@k@50: 0.9640\n",
            "  recall@k@100: 1.0000\n",
            "  precision@k@1: 0.4650\n",
            "  precision@k@5: 0.5351\n",
            "  precision@k@10: 0.5476\n",
            "  precision@k@50: 0.5592\n",
            "  precision@k@100: 0.5598\n",
            "  ndcg@k@1: 0.4650\n",
            "  ndcg@k@5: 0.5636\n",
            "  ndcg@k@10: 0.5936\n",
            "  ndcg@k@50: 0.6444\n",
            "  ndcg@k@100: 0.6503\n",
            "  map: 0.5598\n",
            "  f1@k@1: 0.4650\n",
            "  f1@k@5: 0.5646\n",
            "  f1@k@10: 0.5866\n",
            "  f1@k@50: 0.6084\n",
            "  f1@k@100: 0.6095\n",
            "  r_precision: 0.4650\n",
            "  success_rate@k@1: 0.4650\n",
            "  success_rate@k@5: 0.6490\n",
            "  success_rate@k@10: 0.7410\n",
            "  success_rate@k@50: 0.9640\n",
            "  success_rate@k@100: 1.0000\n",
            "  query_coverage@k@1: 0.4650\n",
            "  query_coverage@k@5: 0.6490\n",
            "  query_coverage@k@10: 0.7410\n",
            "  query_coverage@k@50: 0.9640\n",
            "  query_coverage@k@100: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer, RobertaForMaskedLM, BertForMaskedLM\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"使用デバイス: {device}\")\n",
        "\n",
        "\n",
        "import re\n",
        "\n",
        "def remove_multiline_strings(code):\n",
        "    \"\"\"\n",
        "    Pythonコードからマルチライン文字列（\"\"\" \"\"\" や ''' '''）を削除する関数\n",
        "    \"\"\"\n",
        "    pattern = r'(\"\"\"(.*?)\"\"\"|\\'\\'\\'(.*?)\\'\\'\\')'\n",
        "    return re.sub(pattern, '', code, flags=re.DOTALL)\n",
        "\n",
        "def evaluate_code_search(model, tokenizer, dataset, device, max_examples=100, pool_size=100,\n",
        "                         query_field=\"func_documentation_string\", code_field=\"func_code_string\"):\n",
        "    num_examples = min(len(dataset), max_examples)\n",
        "    all_codes = [remove_multiline_strings(dataset[i][code_field]) for i in range(len(dataset))]\n",
        "    queries = []\n",
        "    correct_code_indices = []\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        ex = dataset[i]\n",
        "        query_text = ex[query_field]\n",
        "        queries.append(query_text)\n",
        "        correct_code_indices.append(i)\n",
        "\n",
        "    all_code_embeddings = []\n",
        "    print(\"データセット全体のコード埋め込みを計算中...\")\n",
        "    for code in all_codes[:num_examples]:\n",
        "        emb = get_cls_embedding(model, tokenizer, code, device)\n",
        "        all_code_embeddings.append(emb)\n",
        "    all_code_embeddings = np.concatenate(all_code_embeddings, axis=0)\n",
        "    print(\"コード埋め込み計算完了.\")\n",
        "\n",
        "    sim_matrices = []\n",
        "    print(\"候補コードプールを作成し、類似度計算中...\")\n",
        "    for i in range(num_examples):\n",
        "        query_embedding = get_cls_embedding(model, tokenizer, queries[i], device).reshape(1, -1)\n",
        "        candidate_pool_embeddings = []\n",
        "        candidate_pool_embeddings.append(all_code_embeddings[correct_code_indices[i]])\n",
        "        incorrect_code_indices = []\n",
        "        while len(incorrect_code_indices) < pool_size - 1:\n",
        "            rand_index = random.randint(0, num_examples - 1)\n",
        "            if rand_index != correct_code_indices[i] and rand_index not in incorrect_code_indices:\n",
        "                incorrect_code_indices.append(rand_index)\n",
        "        for incorrect_index in incorrect_code_indices:\n",
        "            candidate_pool_embeddings.append(all_code_embeddings[incorrect_index])\n",
        "        candidate_pool_embeddings = np.stack(candidate_pool_embeddings, axis=0)\n",
        "        sims = cosine_similarity(query_embedding, candidate_pool_embeddings)[0]\n",
        "        sim_matrices.append(sims)\n",
        "    sim_matrix = np.array(sim_matrices)\n",
        "    print(\"類似度計算完了.\")\n",
        "    metrics = calculate_metrics(sim_matrix)\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def get_cls_embedding(model, tokenizer, text, device, max_length=256):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    if isinstance(model, BertForMaskedLM):\n",
        "        outputs = model.bert(**inputs)\n",
        "    elif isinstance(model, RobertaForMaskedLM):\n",
        "        outputs = model.roberta(**inputs)\n",
        "    else:\n",
        "        raise ValueError(\"Model must be BertForMaskedLM or RobertaForMaskedLM.\")\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "    return cls_embedding.detach().cpu().numpy()\n",
        "\n",
        "def calculate_metrics(sim_matrix, k_values=[1, 5, 10, 50, 100]):\n",
        "    num_queries = sim_matrix.shape[0]\n",
        "    metrics = {\n",
        "        \"mrr\": 0.0,\n",
        "        \"recall@k\": {k: 0.0 for k in k_values},\n",
        "        \"precision@k\": {k: 0.0 for k in k_values},\n",
        "        \"ndcg@k\": {k: 0.0 for k in k_values},\n",
        "        \"map\": 0.0,\n",
        "        \"f1@k\": {k: 0.0 for k in k_values},\n",
        "        \"r_precision\": 0.0,\n",
        "        \"success_rate@k\": {k: 0.0 for k in k_values},\n",
        "        \"query_coverage@k\": {k: 0.0 for k in k_values},\n",
        "    }\n",
        "    for i in range(num_queries):\n",
        "        sims = sim_matrix[i]\n",
        "        ranked_indices = np.argsort(-sims)\n",
        "        correct_rank = np.where(ranked_indices == 0)[0][0] + 1\n",
        "        is_correct_in_top_k = {k: correct_rank <= k for k in k_values}\n",
        "        metrics[\"mrr\"] += 1.0 / correct_rank\n",
        "        for k in k_values:\n",
        "            if correct_rank <= k:\n",
        "                prec = 1.0 / correct_rank\n",
        "                rec = 1.0\n",
        "            else:\n",
        "                prec = 0.0\n",
        "                rec = 0.0\n",
        "            f1 = calculate_f1(prec, rec)\n",
        "            metrics[\"precision@k\"][k] += prec\n",
        "            metrics[\"recall@k\"][k] += rec\n",
        "            metrics[\"f1@k\"][k] += f1\n",
        "            ideal_ranking = [1.0] + [0.0] * (k - 1)\n",
        "            actual_ranking = [1.0 if j == 0 else 0.0 for j in ranked_indices[:k]]\n",
        "            idcg = calculate_dcg(ideal_ranking)\n",
        "            dcg = calculate_dcg(actual_ranking)\n",
        "            metrics[\"ndcg@k\"][k] += dcg / idcg if idcg > 0 else 0.0\n",
        "            metrics[\"success_rate@k\"][k] += 1.0 if is_correct_in_top_k[k] else 0.0\n",
        "            metrics[\"query_coverage@k\"][k] += 1.0 if is_correct_in_top_k[k] else 0.0\n",
        "        metrics[\"map\"] += calculate_average_precision(ranked_indices)\n",
        "        metrics[\"r_precision\"] += calculate_r_precision(ranked_indices)\n",
        "    metrics[\"mrr\"] /= num_queries\n",
        "    metrics[\"map\"] /= num_queries\n",
        "    metrics[\"r_precision\"] /= num_queries\n",
        "    for k in k_values:\n",
        "        metrics[\"recall@k\"][k] /= num_queries\n",
        "        metrics[\"precision@k\"][k] /= num_queries\n",
        "        metrics[\"ndcg@k\"][k] /= num_queries\n",
        "        metrics[\"f1@k\"][k] /= num_queries\n",
        "        metrics[\"success_rate@k\"][k] /= num_queries\n",
        "        metrics[\"query_coverage@k\"][k] /= num_queries\n",
        "    return metrics\n",
        "\n",
        "def calculate_average_precision(ranked_indices):\n",
        "    correct_rank = np.where(ranked_indices == 0)[0][0] + 1\n",
        "    ap = 0.0\n",
        "    for k in range(1, len(ranked_indices) + 1):\n",
        "        if k == correct_rank:\n",
        "            ap += 1.0 / k\n",
        "    return ap\n",
        "\n",
        "def calculate_f1(precision, recall):\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "    return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "def calculate_r_precision(ranked_indices):\n",
        "    r = 1\n",
        "    correct_in_top_r = 0\n",
        "    for i in range(r):\n",
        "        if ranked_indices[i] == 0:\n",
        "            correct_in_top_r += 1\n",
        "    return correct_in_top_r / r\n",
        "\n",
        "def calculate_dcg(ranking):\n",
        "    dcg = 0.0\n",
        "    for i, rel in enumerate(ranking):\n",
        "        dcg += rel / np.log2(i + 2)\n",
        "    return dcg\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\ngoogle/code_x_glue_tc_nl_code_search_adv データセット (Test) をロードします...\")\n",
        "    tc_dataset = load_dataset(\"google/code_x_glue_tc_nl_code_search_adv\", split=\"test\")\n",
        "    subset_tc = tc_dataset.select(range(10000))\n",
        "    model_name1 = \"Shuu12121/CodeMorph-BERT\"\n",
        "    print(f\"\\n{model_name1} を google/code_x_glue_tc_nl_code_search_adv で評価します (候補プールサイズ: 100)...\")\n",
        "    tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n",
        "    model1 = AutoModelForMaskedLM.from_pretrained(model_name1)\n",
        "    model1.to(device)\n",
        "    metrics_tc1 = evaluate_code_search(model1, tokenizer1, subset_tc, device, max_examples=10000, pool_size=100, query_field=\"docstring\", code_field=\"code\")\n",
        "    print(f\"{model_name1} の評価指標 (google/code_x_glue_tc_nl_code_search_adv):\")\n",
        "    for name, value in metrics_tc1.items():\n",
        "        if isinstance(value, dict):\n",
        "            for k, v in value.items():\n",
        "                print(f\"  {name}@{k}: {v:.4f}\")\n",
        "        else:\n",
        "            print(f\"  {name}: {value:.4f}\")\n",
        "    model_name2 = \"microsoft/codebert-base-mlm\"\n",
        "    print(f\"\\n{model_name2} を google/code_x_glue_tc_nl_code_search_adv で評価します (候補プールサイズ: 100)...\")\n",
        "    tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
        "    model2 = AutoModelForMaskedLM.from_pretrained(model_name2)\n",
        "    model2.to(device)\n",
        "    metrics_tc2 = evaluate_code_search(model2, tokenizer2, subset_tc, device, max_examples=10000, pool_size=100, query_field=\"docstring\", code_field=\"code\")\n",
        "    print(f\"{model_name2} の評価指標 (google/code_x_glue_tc_nl_code_search_adv):\")\n",
        "    for name, value in metrics_tc2.items():\n",
        "        if isinstance(value, dict):\n",
        "            for k, v in value.items():\n",
        "                print(f\"  {name}@{k}: {v:.4f}\")\n",
        "        else:\n",
        "            print(f\"  {name}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__9R1W2TsHm0",
        "outputId": "94a382f1-82fd-42f1-d2c2-60e9cdd613a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "使用デバイス: cuda\n",
            "\n",
            "google/code_x_glue_tc_nl_code_search_adv データセット (Test) をロードします...\n",
            "\n",
            "Shuu12121/CodeMorph-BERT を google/code_x_glue_tc_nl_code_search_adv で評価します (候補プールサイズ: 100)...\n",
            "データセット全体のコード埋め込みを計算中...\n",
            "コード埋め込み計算完了.\n",
            "候補コードプールを作成し、類似度計算中...\n",
            "類似度計算完了.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/codebert-base-mlm were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shuu12121/CodeMorph-BERT の評価指標 (google/code_x_glue_tc_nl_code_search_adv):\n",
            "  mrr: 0.2997\n",
            "  recall@k@1: 0.1995\n",
            "  recall@k@5: 0.3928\n",
            "  recall@k@10: 0.4899\n",
            "  recall@k@50: 0.8080\n",
            "  recall@k@100: 1.0000\n",
            "  precision@k@1: 0.1995\n",
            "  precision@k@5: 0.2693\n",
            "  precision@k@10: 0.2821\n",
            "  precision@k@50: 0.2969\n",
            "  precision@k@100: 0.2997\n",
            "  ndcg@k@1: 0.1995\n",
            "  ndcg@k@5: 0.2999\n",
            "  ndcg@k@10: 0.3312\n",
            "  ndcg@k@50: 0.4011\n",
            "  ndcg@k@100: 0.4323\n",
            "  map: 0.2997\n",
            "  f1@k@1: 0.1995\n",
            "  f1@k@5: 0.2998\n",
            "  f1@k@10: 0.3223\n",
            "  f1@k@50: 0.3504\n",
            "  f1@k@100: 0.3558\n",
            "  r_precision: 0.1995\n",
            "  success_rate@k@1: 0.1995\n",
            "  success_rate@k@5: 0.3928\n",
            "  success_rate@k@10: 0.4899\n",
            "  success_rate@k@50: 0.8080\n",
            "  success_rate@k@100: 1.0000\n",
            "  query_coverage@k@1: 0.1995\n",
            "  query_coverage@k@5: 0.3928\n",
            "  query_coverage@k@10: 0.4899\n",
            "  query_coverage@k@50: 0.8080\n",
            "  query_coverage@k@100: 1.0000\n",
            "\n",
            "microsoft/codebert-base-mlm を google/code_x_glue_tc_nl_code_search_adv で評価します (候補プールサイズ: 100)...\n",
            "データセット全体のコード埋め込みを計算中...\n",
            "コード埋め込み計算完了.\n",
            "候補コードプールを作成し、類似度計算中...\n",
            "類似度計算完了.\n",
            "microsoft/codebert-base-mlm の評価指標 (google/code_x_glue_tc_nl_code_search_adv):\n",
            "  mrr: 0.3772\n",
            "  recall@k@1: 0.2697\n",
            "  recall@k@5: 0.4782\n",
            "  recall@k@10: 0.5924\n",
            "  recall@k@50: 0.9036\n",
            "  recall@k@100: 1.0000\n",
            "  precision@k@1: 0.2697\n",
            "  precision@k@5: 0.3456\n",
            "  precision@k@10: 0.3607\n",
            "  precision@k@50: 0.3758\n",
            "  precision@k@100: 0.3772\n",
            "  ndcg@k@1: 0.2697\n",
            "  ndcg@k@5: 0.3785\n",
            "  ndcg@k@10: 0.4154\n",
            "  ndcg@k@50: 0.4847\n",
            "  ndcg@k@100: 0.5004\n",
            "  map: 0.3772\n",
            "  f1@k@1: 0.2697\n",
            "  f1@k@5: 0.3786\n",
            "  f1@k@10: 0.4053\n",
            "  f1@k@50: 0.4338\n",
            "  f1@k@100: 0.4366\n",
            "  r_precision: 0.2697\n",
            "  success_rate@k@1: 0.2697\n",
            "  success_rate@k@5: 0.4782\n",
            "  success_rate@k@10: 0.5924\n",
            "  success_rate@k@50: 0.9036\n",
            "  success_rate@k@100: 1.0000\n",
            "  query_coverage@k@1: 0.2697\n",
            "  query_coverage@k@5: 0.4782\n",
            "  query_coverage@k@10: 0.5924\n",
            "  query_coverage@k@50: 0.9036\n",
            "  query_coverage@k@100: 1.0000\n"
          ]
        }
      ]
    }
  ]
}