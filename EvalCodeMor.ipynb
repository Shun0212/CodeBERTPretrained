{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNfQSk8KW0MEtdWk48p8Y/z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shun0212/CodeBERTPretrained/blob/main/EvalCodeMor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZH5_cdm9-J0Y",
        "outputId": "1d2844b6-993e-46a3-b16b-cd89dfdd8c7b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
        "\n",
        "def get_cls_embedding(model, tokenizer, text, device, max_length=256):\n",
        "    \"\"\"\n",
        "    å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ [CLS] ãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—ã™ã‚‹é–¢æ•°ã€‚\n",
        "    â€»ãƒ¢ãƒ‡ãƒ«ãŒ BERT ç³»ã®å ´åˆã¯ model.bert ã‚’ã€RoBERTa ç³»ã®å ´åˆã¯ model.roberta ã‚’åˆ©ç”¨ã—ã¦å‡ºåŠ›ã‚’å–å¾—ã—ã¾ã™ã€‚\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # ãƒ¢ãƒ‡ãƒ«ã®ç¨®é¡ã«å¿œã˜ã¦å†…éƒ¨ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹\n",
        "    if hasattr(model, \"bert\"):\n",
        "        outputs = model.bert(**inputs)\n",
        "    elif hasattr(model, \"roberta\"):\n",
        "        outputs = model.roberta(**inputs)\n",
        "    else:\n",
        "        raise ValueError(\"Model does not have attribute 'bert' or 'roberta'.\")\n",
        "\n",
        "    # outputs.last_hidden_state: (batch_size, sequence_length, hidden_size)\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] ã¾ãŸã¯æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿\n",
        "    return cls_embedding.detach().cpu().numpy()\n",
        "\n",
        "def calculate_metrics(sim_matrix):\n",
        "    \"\"\"\n",
        "    é¡ä¼¼åº¦è¡Œåˆ—ã‹ã‚‰ MRR, Recall@k, Precision@k ãªã©ã®è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ã€‚\n",
        "    \"\"\"\n",
        "    num_examples = sim_matrix.shape[0]\n",
        "    mrr_total = 0.0\n",
        "    recall_at_1_total = 0.0\n",
        "    recall_at_5_total = 0.0\n",
        "    precision_at_1_total = 0.0\n",
        "    precision_at_5_total = 0.0\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        sims = sim_matrix[i]\n",
        "        ranked_indices = np.argsort(-sims)\n",
        "        rank = np.where(ranked_indices == i)[0][0] + 1\n",
        "        mrr_total += 1.0 / rank\n",
        "\n",
        "        # Recall@k, Precision@k ã®è¨ˆç®—\n",
        "        if i in ranked_indices[:1]:\n",
        "            recall_at_1_total += 1.0\n",
        "            precision_at_1_total += 1.0\n",
        "        if i in ranked_indices[:5]:\n",
        "            recall_at_5_total += 1.0\n",
        "            precision_at_5_total += 1.0\n",
        "\n",
        "    mrr = mrr_total / num_examples\n",
        "    recall_at_1 = recall_at_1_total / num_examples\n",
        "    recall_at_5 = recall_at_5_total / num_examples\n",
        "    precision_at_1 = precision_at_1_total / num_examples\n",
        "    precision_at_5 = precision_at_5_total / num_examples\n",
        "\n",
        "    metrics = {\n",
        "        \"mrr\": mrr,\n",
        "        \"recall@1\": recall_at_1,\n",
        "        \"recall@5\": recall_at_5,\n",
        "        \"precision@1\": precision_at_1,\n",
        "        \"precision@5\": precision_at_5,\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def evaluate_code_search(model, tokenizer, dataset, device, max_examples=100):\n",
        "    \"\"\"\n",
        "    CodeSearchNet ã®ä¸€éƒ¨ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ã£ã¦ã€ã‚¯ã‚¨ãƒªã‹ã‚‰ã‚³ãƒ¼ãƒ‰æ¤œç´¢ã®è©•ä¾¡ã‚’è¡Œã„ã€MRR, Recall@k, Precision@k ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ã€‚\n",
        "\n",
        "    ... ( Docstring ã¯å¤‰æ›´ãªã— ) ...\n",
        "    \"\"\"\n",
        "    codes = []\n",
        "    queries = []\n",
        "    num_examples = min(len(dataset), max_examples)\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        ex = dataset[i]\n",
        "        # ã‚­ãƒ¼ãŒã©ã®åå‰ã«ãªã£ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦æŠ½å‡ºã™ã‚‹\n",
        "        if \"func_code_string\" in ex:\n",
        "            code_text = ex[\"func_code_string\"]\n",
        "        elif \"code\" in ex:\n",
        "            code_text = ex[\"code\"]\n",
        "        else:\n",
        "            raise KeyError(f\"ã‚µãƒ³ãƒ—ãƒ« {i} ã«ã‚³ãƒ¼ãƒ‰ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ©ç”¨å¯èƒ½ãªã‚­ãƒ¼: {list(ex.keys())}\")\n",
        "\n",
        "        if \"func_documentation_string\" in ex:\n",
        "            query_text = ex[\"func_documentation_string\"]\n",
        "        elif \"docstring\" in ex:\n",
        "            query_text = ex[\"docstring\"]\n",
        "        else:\n",
        "            raise KeyError(f\"ã‚µãƒ³ãƒ—ãƒ« {i} ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ–‡å­—åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ©ç”¨å¯èƒ½ãªã‚­ãƒ¼: {list(ex.keys())}\")\n",
        "\n",
        "        codes.append(code_text)\n",
        "        queries.append(query_text)\n",
        "\n",
        "    # å„ã‚³ãƒ¼ãƒ‰ã¨ã‚¯ã‚¨ãƒªã®åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—\n",
        "    code_embeddings = []\n",
        "    query_embeddings = []\n",
        "\n",
        "    for code in codes:\n",
        "        emb = get_cls_embedding(model, tokenizer, code, device)\n",
        "        code_embeddings.append(emb)\n",
        "    for query in queries:\n",
        "        emb = get_cls_embedding(model, tokenizer, query, device)\n",
        "        query_embeddings.append(emb)\n",
        "\n",
        "    # å„åŸ‹ã‚è¾¼ã¿ã¯ shape (1, hidden_size) ã«ãªã£ã¦ã„ã‚‹ã®ã§é€£çµ\n",
        "    code_embeddings = np.concatenate(code_embeddings, axis=0)  # shape: (num_examples, hidden_size)\n",
        "    query_embeddings = np.concatenate(query_embeddings, axis=0)\n",
        "\n",
        "    # ã‚¯ã‚¨ãƒªã¨ã‚³ãƒ¼ãƒ‰ã®é–“ã® cosine similarity è¡Œåˆ—ã‚’è¨ˆç®—\n",
        "    sim_matrix = cosine_similarity(query_embeddings, code_embeddings)\n",
        "\n",
        "    # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—\n",
        "    metrics = calculate_metrics(sim_matrix)\n",
        "    return metrics\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # CodeSearchNet ã® Python éƒ¨åˆ†ã®ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰\n",
        "    print(\"CodeSearchNet ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™...\")\n",
        "    dataset = load_dataset(\"code_search_net\", \"python\", split=\"test\", trust_remote_code=True)\n",
        "    # ãƒ‡ãƒ¢ç”¨ã«ä¸Šä½ 100 ä»¶ã‚’è©•ä¾¡å¯¾è±¡ã«ã™ã‚‹\n",
        "    subset = dataset.select(range(1000))\n",
        "\n",
        "    ### CodeMorph-BERT ã®è©•ä¾¡ ###\n",
        "    model_name1 = \"Shuu12121/CodeMorph-BERT\"\n",
        "    print(f\"\\n{model_name1} ã‚’è©•ä¾¡ã—ã¾ã™...\")\n",
        "    tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n",
        "    model1 = AutoModelForMaskedLM.from_pretrained(model_name1)\n",
        "    model1.to(device)\n",
        "    metrics1 = evaluate_code_search(model1, tokenizer1, subset, device, max_examples=23107) # max_examples ã¯é©å®œèª¿æ•´\n",
        "    print(f\"CodeMorph-BERT ã®è©•ä¾¡æŒ‡æ¨™:\")\n",
        "    for name, value in metrics1.items():\n",
        "        print(f\"  {name}: {value:.4f}\")\n",
        "\n",
        "    ### Microsoft CodeBERT ã®è©•ä¾¡ ###\n",
        "    model_name2 = \"microsoft/codebert-base-mlm\"\n",
        "    print(f\"\\n{model_name2} ã‚’è©•ä¾¡ã—ã¾ã™...\")\n",
        "    tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
        "    model2 = AutoModelForMaskedLM.from_pretrained(model_name2)\n",
        "    model2.to(device)\n",
        "    metrics2 = evaluate_code_search(model2, tokenizer2, subset, device, max_examples=23107) # max_examples ã¯é©å®œèª¿æ•´\n",
        "    print(f\"CodeBERT ã®è©•ä¾¡æŒ‡æ¨™:\")\n",
        "    for name, value in metrics2.items():\n",
        "        print(f\"  {name}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NK0JTEvJ1dHc",
        "outputId": "50378dd7-6c0f-464b-9cf3-425ba97c5f79"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: cuda\n",
            "CodeSearchNet ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shuu12121/CodeMorph-BERT ã‚’è©•ä¾¡ã—ã¾ã™...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CodeMorph-BERT ã®è©•ä¾¡æŒ‡æ¨™:\n",
            "  mrr: 0.7668\n",
            "  recall@1: 0.6930\n",
            "  recall@5: 0.8500\n",
            "  precision@1: 0.6930\n",
            "  precision@5: 0.8500\n",
            "\n",
            "microsoft/codebert-base-mlm ã‚’è©•ä¾¡ã—ã¾ã™...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/codebert-base-mlm were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CodeBERT ã®è©•ä¾¡æŒ‡æ¨™:\n",
            "  mrr: 0.6766\n",
            "  recall@1: 0.6180\n",
            "  recall@5: 0.7370\n",
            "  precision@1: 0.6180\n",
            "  precision@5: 0.7370\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer, RobertaForMaskedLM, BertForMaskedLM\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "\n",
        "# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
        "\n",
        "def get_cls_embedding(model, tokenizer, text, device, max_length=256):\n",
        "    \"\"\"\n",
        "    å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ [CLS] ãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—ã™ã‚‹é–¢æ•°ã€‚\n",
        "    â€»ãƒ¢ãƒ‡ãƒ«ãŒ BERT ç³»ã®å ´åˆã¯ model.bert ã‚’ã€RoBERTa ç³»ã®å ´åˆã¯ model.roberta ã‚’åˆ©ç”¨ã—ã¦å‡ºåŠ›ã‚’å–å¾—ã—ã¾ã™ã€‚\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # ãƒ¢ãƒ‡ãƒ«ã®ç¨®é¡ã«å¿œã˜ã¦å†…éƒ¨ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹\n",
        "    if isinstance(model, BertForMaskedLM): # BERT ç³»ãƒ¢ãƒ‡ãƒ«ã®åˆ¤å®šã‚’ä¿®æ­£\n",
        "        outputs = model.bert(**inputs)\n",
        "    elif isinstance(model, RobertaForMaskedLM): # RoBERTa ç³»ãƒ¢ãƒ‡ãƒ«ã®åˆ¤å®šã‚’ä¿®æ­£\n",
        "        outputs = model.roberta(**inputs)\n",
        "    else:\n",
        "        raise ValueError(\"Model must be BertForMaskedLM or RobertaForMaskedLM.\") # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¿®æ­£\n",
        "\n",
        "    # outputs.last_hidden_state: (batch_size, sequence_length, hidden_size)\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] ã¾ãŸã¯æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿\n",
        "    return cls_embedding.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def calculate_metrics(sim_matrix, k_values=[1, 5, 10, 50, 100]):\n",
        "    num_queries = sim_matrix.shape[0]\n",
        "    metrics = {\n",
        "        \"mrr\": 0.0,\n",
        "        \"recall@k\": {k: 0.0 for k in k_values},\n",
        "        \"precision@k\": {k: 0.0 for k in k_values},\n",
        "        \"ndcg@k\": {k: 0.0 for k in k_values},\n",
        "        \"map\": 0.0,\n",
        "        \"f1@k\": {k: 0.0 for k in k_values},\n",
        "        \"r_precision\": 0.0,\n",
        "        \"success_rate@k\": {k: 0.0 for k in k_values},\n",
        "        \"query_coverage@k\": {k: 0.0 for k in k_values},\n",
        "    }\n",
        "\n",
        "    for i in range(num_queries):\n",
        "        sims = sim_matrix[i]\n",
        "        ranked_indices = np.argsort(-sims)\n",
        "        correct_rank = np.where(ranked_indices == 0)[0][0] + 1\n",
        "        is_correct_in_top_k = {k: correct_rank <= k for k in k_values}\n",
        "\n",
        "        # MRR\n",
        "        metrics[\"mrr\"] += 1.0 / correct_rank\n",
        "\n",
        "        # å„ k ã«å¯¾ã—ã¦ã‚¯ã‚¨ãƒªã”ã¨ã® precision, recall, f1 ã‚’è¨ˆç®—\n",
        "        for k in k_values:\n",
        "            if correct_rank <= k:\n",
        "                # æ­£è§£ãŒãƒˆãƒƒãƒ—kã«å«ã¾ã‚Œã‚‹å ´åˆ\n",
        "                prec = 1.0 / correct_rank\n",
        "                rec = 1.0  # å€™è£œãŒ1ä»¶ã®å ´åˆã€æ­£è§£ãŒå«ã¾ã‚Œã¦ã„ã‚Œã° recall ã¯ 1.0\n",
        "            else:\n",
        "                prec = 0.0\n",
        "                rec = 0.0\n",
        "            f1 = calculate_f1(prec, rec)\n",
        "\n",
        "            metrics[\"precision@k\"][k] += prec\n",
        "            metrics[\"recall@k\"][k] += rec\n",
        "            metrics[\"f1@k\"][k] += f1\n",
        "\n",
        "            # NDCG\n",
        "            ideal_ranking = [1.0] + [0.0] * (k - 1)\n",
        "            actual_ranking = [1.0 if j == 0 else 0.0 for j in ranked_indices[:k]]\n",
        "            idcg = calculate_dcg(ideal_ranking)\n",
        "            dcg = calculate_dcg(actual_ranking)\n",
        "            metrics[\"ndcg@k\"][k] += dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "            # Success Rate, Query Coverage\n",
        "            metrics[\"success_rate@k\"][k] += 1.0 if is_correct_in_top_k[k] else 0.0\n",
        "            metrics[\"query_coverage@k\"][k] += 1.0 if is_correct_in_top_k[k] else 0.0\n",
        "\n",
        "        # MAP, R-Precision\n",
        "        metrics[\"map\"] += calculate_average_precision(ranked_indices)\n",
        "        metrics[\"r_precision\"] += calculate_r_precision(ranked_indices)\n",
        "\n",
        "    # å„æŒ‡æ¨™ã‚’ã‚¯ã‚¨ãƒªæ•°ã§å¹³å‡åŒ–\n",
        "    metrics[\"mrr\"] /= num_queries\n",
        "    metrics[\"map\"] /= num_queries\n",
        "    metrics[\"r_precision\"] /= num_queries\n",
        "    for k in k_values:\n",
        "        metrics[\"recall@k\"][k] /= num_queries\n",
        "        metrics[\"precision@k\"][k] /= num_queries\n",
        "        metrics[\"ndcg@k\"][k] /= num_queries\n",
        "        metrics[\"f1@k\"][k] /= num_queries\n",
        "        metrics[\"success_rate@k\"][k] /= num_queries\n",
        "        metrics[\"query_coverage@k\"][k] /= num_queries\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def calculate_average_precision(ranked_indices): # â˜… MAP è¨ˆç®—ç”¨é–¢æ•°ã‚’è¿½åŠ \n",
        "    \"\"\"\n",
        "    Average Precision (AP) ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ã€‚\n",
        "    \"\"\"\n",
        "    correct_rank = np.where(ranked_indices == 0)[0][0] + 1\n",
        "    ap = 0.0\n",
        "    for k in range(1, len(ranked_indices) + 1):\n",
        "        if k == correct_rank: # kç•ªç›®ã«æ­£è§£ã‚³ãƒ¼ãƒ‰ãŒç¾ã‚ŒãŸå ´åˆã®ã¿ Precision ã‚’åŠ ç®—\n",
        "            ap += 1.0 / k\n",
        "    return ap\n",
        "\n",
        "\n",
        "def calculate_f1(precision, recall): # â˜… F1å€¤è¨ˆç®—ç”¨é–¢æ•°ã‚’è¿½åŠ \n",
        "    \"\"\"\n",
        "    F1å€¤ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•° (Precision ã¨ Recall ã‹ã‚‰è¨ˆç®—)ã€‚\n",
        "    \"\"\"\n",
        "    if precision + recall == 0: # Precision, Recall ãŒå…±ã« 0 ã®å ´åˆ\n",
        "        return 0.0\n",
        "    return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "\n",
        "def calculate_r_precision(ranked_indices): # â˜… R-Precision è¨ˆç®—ç”¨é–¢æ•°ã‚’è¿½åŠ \n",
        "    \"\"\"\n",
        "    R-Precision ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•° (R=1 ã§å›ºå®š: å€™è£œãƒ—ãƒ¼ãƒ«ã«æ­£è§£ã‚³ãƒ¼ãƒ‰ã¯1ã¤ã®ã¿)ã€‚\n",
        "    \"\"\"\n",
        "    r = 1 # æ­£è§£ã‚³ãƒ¼ãƒ‰æ•° (å€™è£œãƒ—ãƒ¼ãƒ«ã«1ã¤ã®ã¿)\n",
        "    correct_in_top_r = 0\n",
        "    for i in range(r): # ä¸Šä½ r ä»¶ã¾ã§ç¢ºèª\n",
        "        if ranked_indices[i] == 0: # æ­£è§£ã‚³ãƒ¼ãƒ‰ãŒãƒ©ãƒ³ã‚¯ã‚¤ãƒ³\n",
        "            correct_in_top_r += 1\n",
        "    return correct_in_top_r / r # R-Precision ã¯é©åˆç‡\n",
        "\n",
        "def calculate_dcg(ranking):\n",
        "    \"\"\"\n",
        "    Discounted Cumulative Gain (DCG) ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ã€‚\n",
        "    \"\"\"\n",
        "    dcg = 0.0\n",
        "    for i, rel in enumerate(ranking):\n",
        "        dcg += rel / np.log2(i + 2) # é †ä½ i+1 ã®å‰²å¼•ç‡: 1/log2(i+2)\n",
        "    return dcg\n",
        "\n",
        "\n",
        "def evaluate_code_search(model, tokenizer, dataset, device, max_examples=100, pool_size=100):\n",
        "    \"\"\"\n",
        "    CodeSearchNet ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ã£ã¦ã€ã‚¯ã‚¨ãƒªã‹ã‚‰ã‚³ãƒ¼ãƒ‰æ¤œç´¢ã®å³å¯†ãªè©•ä¾¡ã‚’è¡Œã„ã€\n",
        "    MRR, Recall@k, Precision@k, NDCG@k ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•° (å€™è£œã‚³ãƒ¼ãƒ‰ãƒ—ãƒ¼ãƒ«ä½¿ç”¨)ã€‚\n",
        "\n",
        "    Args:\n",
        "        model, tokenizer: è©•ä¾¡å¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼\n",
        "        dataset: Hugging Face Datasets ã® CodeSearchNet ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
        "        device: \"cpu\" ã¾ãŸã¯ \"cuda\"\n",
        "        max_examples: è©•ä¾¡ã«ä½¿ã†ã‚¯ã‚¨ãƒªã®æœ€å¤§æ•°\n",
        "        pool_size: å„ã‚¯ã‚¨ãƒªã«å¯¾ã™ã‚‹å€™è£œã‚³ãƒ¼ãƒ‰ãƒ—ãƒ¼ãƒ«ã®ã‚µã‚¤ã‚º (æ­£è§£ã‚³ãƒ¼ãƒ‰1ã¤ + ä¸æ­£è§£ã‚³ãƒ¼ãƒ‰ pool_size-1)\n",
        "\n",
        "    Returns:\n",
        "        metrics: è©•ä¾¡æŒ‡æ¨™ (MRR, Recall@k, Precision@k, NDCG@k) ã‚’å«ã‚€è¾æ›¸\n",
        "    \"\"\"\n",
        "    num_examples = min(len(dataset), max_examples)\n",
        "    all_codes = [dataset[i][\"func_code_string\"] for i in range(len(dataset))] # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®ã‚³ãƒ¼ãƒ‰ã‚’å–å¾— (ä¸æ­£è§£å€™è£œã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç”¨)\n",
        "    queries = []\n",
        "    correct_code_indices = [] # å„ã‚¯ã‚¨ãƒªã«å¯¾å¿œã™ã‚‹æ­£è§£ã‚³ãƒ¼ãƒ‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿æŒ\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        ex = dataset[i]\n",
        "        query_text = ex[\"func_documentation_string\"]\n",
        "        queries.append(query_text)\n",
        "        correct_code_indices.append(i) # æ­£è§£ã‚³ãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨˜éŒ²\n",
        "\n",
        "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®ã‚³ãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿ã‚’äº‹å‰ã«è¨ˆç®— (æ™‚é–“çŸ­ç¸®ã®ãŸã‚)\n",
        "    all_code_embeddings = []\n",
        "    print(\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®ã‚³ãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿ã‚’è¨ˆç®—...\")\n",
        "    for code in all_codes[:num_examples]: # max_examples ã¾ã§ã«åˆ¶é™\n",
        "        emb = get_cls_embedding(model, tokenizer, code, device)\n",
        "        all_code_embeddings.append(emb)\n",
        "    all_code_embeddings = np.concatenate(all_code_embeddings, axis=0) # shape: (num_examples, hidden_size)\n",
        "    print(\"ã‚³ãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿è¨ˆç®—å®Œäº†.\")\n",
        "\n",
        "\n",
        "    sim_matrices = [] # å„ã‚¯ã‚¨ãƒªã®é¡ä¼¼åº¦è¡Œåˆ—ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ\n",
        "    print(\"å€™è£œã‚³ãƒ¼ãƒ‰ãƒ—ãƒ¼ãƒ«ã‚’ä½œæˆã—ã€é¡ä¼¼åº¦è¡Œåˆ—ã‚’è¨ˆç®—...\")\n",
        "    for i in range(num_examples): # å„ã‚¯ã‚¨ãƒªã”ã¨ã«å€™è£œã‚³ãƒ¼ãƒ‰ãƒ—ãƒ¼ãƒ«ã‚’ä½œæˆ\n",
        "        query_embedding = get_cls_embedding(model, tokenizer, queries[i], device).reshape(1, -1) # ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ã‚’è¨ˆç®— # â˜…reshape ã‚’è¿½åŠ \n",
        "\n",
        "        candidate_pool_embeddings = []\n",
        "        candidate_pool_embeddings.append(all_code_embeddings[correct_code_indices[i]]) # ãƒ—ãƒ¼ãƒ«å…ˆé ­ã«æ­£è§£ã‚³ãƒ¼ãƒ‰ã®åŸ‹ã‚è¾¼ã¿ã‚’è¿½åŠ \n",
        "\n",
        "        # ä¸æ­£è§£ã‚³ãƒ¼ãƒ‰ã‚’å€™è£œãƒ—ãƒ¼ãƒ«ã«è¿½åŠ  (ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)\n",
        "        incorrect_code_indices = []\n",
        "        while len(incorrect_code_indices) < pool_size - 1:\n",
        "            rand_index = random.randint(0, num_examples - 1) # ä¸æ­£è§£ã‚³ãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ (max_examplesã¾ã§)\n",
        "            if rand_index != correct_code_indices[i] and rand_index not in incorrect_code_indices: # æ­£è§£ã‚³ãƒ¼ãƒ‰ã¨é‡è¤‡ã—ãªã„ã‚ˆã†ã«ãƒã‚§ãƒƒã‚¯\n",
        "                incorrect_code_indices.append(rand_index)\n",
        "        for incorrect_index in incorrect_code_indices:\n",
        "            candidate_pool_embeddings.append(all_code_embeddings[incorrect_index])\n",
        "        candidate_pool_embeddings = np.stack(candidate_pool_embeddings, axis=0) # shape: (pool_size, hidden_size) # â˜…np.stack ã«å¤‰æ›´\n",
        "\n",
        "\n",
        "\n",
        "        # ã‚¯ã‚¨ãƒªã¨å€™è£œã‚³ãƒ¼ãƒ‰ãƒ—ãƒ¼ãƒ«é–“ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—\n",
        "        sims = cosine_similarity(query_embedding, candidate_pool_embeddings)[0] # shape: (pool_size,)\n",
        "        sim_matrices.append(sims) # shape: (num_queries, pool_size)\n",
        "    sim_matrix = np.array(sim_matrices) # shape: (num_queries, pool_size)\n",
        "    print(\"é¡ä¼¼åº¦è¡Œåˆ—è¨ˆç®—å®Œäº†.\")\n",
        "\n",
        "\n",
        "    # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—\n",
        "    metrics = calculate_metrics(sim_matrix)\n",
        "    return metrics\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # CodeSearchNet ã® Python éƒ¨åˆ†ã®ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰\n",
        "    print(\"CodeSearchNet ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™...\")\n",
        "    dataset = load_dataset(\"code_search_net\", \"python\", split=\"test\", trust_remote_code=True)\n",
        "    # ãƒ‡ãƒ¢ç”¨ã«ä¸Šä½ 1000 ä»¶ã‚’è©•ä¾¡å¯¾è±¡ã«ã™ã‚‹\n",
        "    subset = dataset.select(range(1000)) # è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’èª¿æ•´\n",
        "\n",
        "    ### CodeMorph-BERT ã®è©•ä¾¡ ###\n",
        "    model_name1 = \"Shuu12121/CodeMorph-BERT\"\n",
        "    print(f\"\\n{model_name1} ã‚’è©•ä¾¡ã—ã¾ã™ (å€™è£œãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º: 100)...\")\n",
        "    tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n",
        "    model1 = AutoModelForMaskedLM.from_pretrained(model_name1) # AutoModelForMaskedLM ã‚’ä½¿ç”¨\n",
        "    model1.to(device)\n",
        "    metrics1 = evaluate_code_search(model1, tokenizer1, subset, device, max_examples=1000, pool_size=100) # max_examples, pool_size ã‚’èª¿æ•´\n",
        "    print(f\"CodeMorph-BERT ã®è©•ä¾¡æŒ‡æ¨™ (å€™è£œãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º: 100):\")\n",
        "    for name, value_dict in metrics1.items(): # metrics1 ã¯è¾æ›¸ã®ä¸­ã«è¾æ›¸ã‚’æŒã¤æ§‹é€  (metrics1.items() ã§å¤–å´ã®è¾æ›¸ã‚’iterate)\n",
        "        if isinstance(value_dict, dict): # Recall@k, Precision@k, NDCG@k ã®å ´åˆ (è¾æ›¸å‹)\n",
        "            for k, v in value_dict.items(): # value_dict.items() ã§å†…å´ã®è¾æ›¸ã‚’iterate\n",
        "                print(f\"  {name}@{k}: {v:.4f}\")\n",
        "        else: # MRR ã®å ´åˆ (è¾æ›¸å‹ã§ã¯ãªã„)\n",
        "            print(f\"  {name}: {value_dict:.4f}\")\n",
        "\n",
        "\n",
        "    ### Microsoft CodeBERT (microsoft/codebert-base-mlm) ã®è©•ä¾¡ ###\n",
        "    model_name2 = \"microsoft/codebert-base-mlm\"\n",
        "    print(f\"\\n{model_name2} ã‚’è©•ä¾¡ã—ã¾ã™ (å€™è£œãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º: 100)...\")\n",
        "    tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
        "    model2 = AutoModelForMaskedLM.from_pretrained(model_name2) # AutoModelForMaskedLM ã‚’ä½¿ç”¨\n",
        "    model2.to(device)\n",
        "    metrics2 = evaluate_code_search(model2, tokenizer2, subset, device, max_examples=1000, pool_size=100) # max_examples, pool_size ã‚’èª¿æ•´\n",
        "    print(f\"CodeBERT (microsoft/codebert-base-mlm) ã®è©•ä¾¡æŒ‡æ¨™ (å€™è£œãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º: 100):\")\n",
        "    for name, value_dict in metrics2.items(): # metrics2 ã¯è¾æ›¸ã®ä¸­ã«è¾æ›¸ã‚’æŒã¤æ§‹é€  (metrics2.items() ã§å¤–å´ã®è¾æ›¸ã‚’iterate)\n",
        "        if isinstance(value_dict, dict): # Recall@k, Precision@k, NDCG@k ã®å ´åˆ (è¾æ›¸å‹)\n",
        "            for k, v in value_dict.items(): # value_dict.items() ã§å†…å´ã®è¾æ›¸ã‚’iterate\n",
        "                print(f\"  {name}@{k}: {v:.4f}\")\n",
        "        else: # MRR ã®å ´åˆ (è¾æ›¸å‹ã§ã¯ãªã„)\n",
        "            print(f\"  {name}: {value_dict:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oawz5-085ybw",
        "outputId": "b60a17cf-562c-4bb0-f89e-98695fde7171"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: cuda\n",
            "CodeSearchNet ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™...\n",
            "\n",
            "Shuu12121/CodeMorph-BERT ã‚’è©•ä¾¡ã—ã¾ã™ (å€™è£œãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º: 100)...\n",
            "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®ã‚³ãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿ã‚’è¨ˆç®—...\n",
            "ã‚³ãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿è¨ˆç®—å®Œäº†.\n",
            "å€™è£œã‚³ãƒ¼ãƒ‰ãƒ—ãƒ¼ãƒ«ã‚’ä½œæˆã—ã€é¡ä¼¼åº¦è¡Œåˆ—ã‚’è¨ˆç®—...\n",
            "é¡ä¼¼åº¦è¡Œåˆ—è¨ˆç®—å®Œäº†.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/codebert-base-mlm were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CodeMorph-BERT ã®è©•ä¾¡æŒ‡æ¨™ (å€™è£œãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º: 100):\n",
            "  mrr: 0.8929\n",
            "  recall@k@1: 0.8520\n",
            "  recall@k@5: 0.9460\n",
            "  recall@k@10: 0.9660\n",
            "  recall@k@50: 0.9910\n",
            "  recall@k@100: 1.0000\n",
            "  precision@k@1: 0.8520\n",
            "  precision@k@5: 0.8885\n",
            "  precision@k@10: 0.8912\n",
            "  precision@k@50: 0.8927\n",
            "  precision@k@100: 0.8929\n",
            "  ndcg@k@1: 0.8520\n",
            "  ndcg@k@5: 0.9029\n",
            "  ndcg@k@10: 0.9094\n",
            "  ndcg@k@50: 0.9155\n",
            "  ndcg@k@100: 0.9170\n",
            "  map: 0.8929\n",
            "  f1@k@1: 0.8520\n",
            "  f1@k@5: 0.9036\n",
            "  f1@k@10: 0.9084\n",
            "  f1@k@50: 0.9113\n",
            "  f1@k@100: 0.9115\n",
            "  r_precision: 0.8520\n",
            "  success_rate@k@1: 0.8520\n",
            "  success_rate@k@5: 0.9460\n",
            "  success_rate@k@10: 0.9660\n",
            "  success_rate@k@50: 0.9910\n",
            "  success_rate@k@100: 1.0000\n",
            "  query_coverage@k@1: 0.8520\n",
            "  query_coverage@k@5: 0.9460\n",
            "  query_coverage@k@10: 0.9660\n",
            "  query_coverage@k@50: 0.9910\n",
            "  query_coverage@k@100: 1.0000\n",
            "\n",
            "microsoft/codebert-base-mlm ã‚’è©•ä¾¡ã—ã¾ã™ (å€™è£œãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º: 100)...\n",
            "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®ã‚³ãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿ã‚’è¨ˆç®—...\n",
            "ã‚³ãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿è¨ˆç®—å®Œäº†.\n",
            "å€™è£œã‚³ãƒ¼ãƒ‰ãƒ—ãƒ¼ãƒ«ã‚’ä½œæˆã—ã€é¡ä¼¼åº¦è¡Œåˆ—ã‚’è¨ˆç®—...\n",
            "é¡ä¼¼åº¦è¡Œåˆ—è¨ˆç®—å®Œäº†.\n",
            "CodeBERT (microsoft/codebert-base-mlm) ã®è©•ä¾¡æŒ‡æ¨™ (å€™è£œãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º: 100):\n",
            "  mrr: 0.8064\n",
            "  recall@k@1: 0.7560\n",
            "  recall@k@5: 0.8590\n",
            "  recall@k@10: 0.9050\n",
            "  recall@k@50: 0.9880\n",
            "  recall@k@100: 1.0000\n",
            "  precision@k@1: 0.7560\n",
            "  precision@k@5: 0.7956\n",
            "  precision@k@10: 0.8015\n",
            "  precision@k@50: 0.8062\n",
            "  precision@k@100: 0.8064\n",
            "  ndcg@k@1: 0.7560\n",
            "  ndcg@k@5: 0.8114\n",
            "  ndcg@k@10: 0.8261\n",
            "  ndcg@k@50: 0.8455\n",
            "  ndcg@k@100: 0.8475\n",
            "  map: 0.8064\n",
            "  f1@k@1: 0.7560\n",
            "  f1@k@5: 0.8119\n",
            "  f1@k@10: 0.8224\n",
            "  f1@k@50: 0.8312\n",
            "  f1@k@100: 0.8316\n",
            "  r_precision: 0.7560\n",
            "  success_rate@k@1: 0.7560\n",
            "  success_rate@k@5: 0.8590\n",
            "  success_rate@k@10: 0.9050\n",
            "  success_rate@k@50: 0.9880\n",
            "  success_rate@k@100: 1.0000\n",
            "  query_coverage@k@1: 0.7560\n",
            "  query_coverage@k@5: 0.8590\n",
            "  query_coverage@k@10: 0.9050\n",
            "  query_coverage@k@50: 0.9880\n",
            "  query_coverage@k@100: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer, RobertaForMaskedLM, BertForMaskedLM\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import random\n",
        "\n",
        "# ãƒ‡ãƒã‚¤ã‚¹è¨­å®š\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}\")\n",
        "\n",
        "def get_cls_embedding(model, tokenizer, text, device, max_length=256):\n",
        "    \"\"\"\n",
        "    å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ [CLS] ãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—ã™ã‚‹é–¢æ•°ã€‚\n",
        "    â€»ãƒ¢ãƒ‡ãƒ«ãŒ BERT ç³»ã®å ´åˆã¯ model.bert ã‚’ã€RoBERTa ç³»ã®å ´åˆã¯ model.roberta ã‚’åˆ©ç”¨ã—ã¦å‡ºåŠ›ã‚’å–å¾—ã—ã¾ã™ã€‚\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    if isinstance(model, BertForMaskedLM):\n",
        "        outputs = model.bert(**inputs)\n",
        "    elif isinstance(model, RobertaForMaskedLM):\n",
        "        outputs = model.roberta(**inputs)\n",
        "    else:\n",
        "        raise ValueError(\"Model must be BertForMaskedLM or RobertaForMaskedLM.\")\n",
        "\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
        "    return cls_embedding.detach().cpu().numpy()\n",
        "\n",
        "def calculate_metrics(sim_matrix, k_values=[1, 5, 10, 50, 100]):\n",
        "    num_queries = sim_matrix.shape[0]\n",
        "    metrics = {\n",
        "        \"mrr\": 0.0,\n",
        "        \"recall@k\": {k: 0.0 for k in k_values},\n",
        "        \"precision@k\": {k: 0.0 for k in k_values},\n",
        "        \"ndcg@k\": {k: 0.0 for k in k_values},\n",
        "        \"map\": 0.0,\n",
        "        \"f1@k\": {k: 0.0 for k in k_values},\n",
        "        \"r_precision\": 0.0,\n",
        "        \"success_rate@k\": {k: 0.0 for k in k_values},\n",
        "        \"query_coverage@k\": {k: 0.0 for k in k_values},\n",
        "    }\n",
        "\n",
        "    for i in range(num_queries):\n",
        "        sims = sim_matrix[i]\n",
        "        ranked_indices = np.argsort(-sims)\n",
        "        correct_rank = np.where(ranked_indices == 0)[0][0] + 1\n",
        "        is_correct_in_top_k = {k: correct_rank <= k for k in k_values}\n",
        "\n",
        "        # MRR\n",
        "        metrics[\"mrr\"] += 1.0 / correct_rank\n",
        "\n",
        "        # å„ k ã«å¯¾ã—ã¦ã‚¯ã‚¨ãƒªã”ã¨ã® precision, recall, f1 ã‚’è¨ˆç®—\n",
        "        for k in k_values:\n",
        "            if correct_rank <= k:\n",
        "                # æ­£è§£ãŒãƒˆãƒƒãƒ—kã«å«ã¾ã‚Œã‚‹å ´åˆ\n",
        "                prec = 1.0 / correct_rank\n",
        "                rec = 1.0  # å€™è£œãŒ1ä»¶ã®å ´åˆã€æ­£è§£ãŒå«ã¾ã‚Œã¦ã„ã‚Œã° recall ã¯ 1.0\n",
        "            else:\n",
        "                prec = 0.0\n",
        "                rec = 0.0\n",
        "            f1 = calculate_f1(prec, rec)\n",
        "\n",
        "            metrics[\"precision@k\"][k] += prec\n",
        "            metrics[\"recall@k\"][k] += rec\n",
        "            metrics[\"f1@k\"][k] += f1\n",
        "\n",
        "            # NDCG\n",
        "            ideal_ranking = [1.0] + [0.0] * (k - 1)\n",
        "            actual_ranking = [1.0 if j == 0 else 0.0 for j in ranked_indices[:k]]\n",
        "            idcg = calculate_dcg(ideal_ranking)\n",
        "            dcg = calculate_dcg(actual_ranking)\n",
        "            metrics[\"ndcg@k\"][k] += dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "            # Success Rate, Query Coverage\n",
        "            metrics[\"success_rate@k\"][k] += 1.0 if is_correct_in_top_k[k] else 0.0\n",
        "            metrics[\"query_coverage@k\"][k] += 1.0 if is_correct_in_top_k[k] else 0.0\n",
        "\n",
        "        # MAP, R-Precision\n",
        "        metrics[\"map\"] += calculate_average_precision(ranked_indices)\n",
        "        metrics[\"r_precision\"] += calculate_r_precision(ranked_indices)\n",
        "\n",
        "    # å„æŒ‡æ¨™ã‚’ã‚¯ã‚¨ãƒªæ•°ã§å¹³å‡åŒ–\n",
        "    metrics[\"mrr\"] /= num_queries\n",
        "    metrics[\"map\"] /= num_queries\n",
        "    metrics[\"r_precision\"] /= num_queries\n",
        "    for k in k_values:\n",
        "        metrics[\"recall@k\"][k] /= num_queries\n",
        "        metrics[\"precision@k\"][k] /= num_queries\n",
        "        metrics[\"ndcg@k\"][k] /= num_queries\n",
        "        metrics[\"f1@k\"][k] /= num_queries\n",
        "        metrics[\"success_rate@k\"][k] /= num_queries\n",
        "        metrics[\"query_coverage@k\"][k] /= num_queries\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def calculate_average_precision(ranked_indices):\n",
        "    correct_rank = np.where(ranked_indices == 0)[0][0] + 1\n",
        "    ap = 0.0\n",
        "    for k in range(1, len(ranked_indices) + 1):\n",
        "        if k == correct_rank:\n",
        "            ap += 1.0 / k\n",
        "    return ap\n",
        "\n",
        "def calculate_f1(precision, recall):\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "    return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "def calculate_r_precision(ranked_indices):\n",
        "    r = 1\n",
        "    correct_in_top_r = 0\n",
        "    for i in range(r):\n",
        "        if ranked_indices[i] == 0:\n",
        "            correct_in_top_r += 1\n",
        "    return correct_in_top_r / r\n",
        "\n",
        "def calculate_dcg(ranking):\n",
        "    dcg = 0.0\n",
        "    for i, rel in enumerate(ranking):\n",
        "        dcg += rel / np.log2(i + 2)\n",
        "    return dcg\n",
        "\n",
        "def evaluate_code_search(model, tokenizer, dataset, device, max_examples=100, pool_size=100,\n",
        "                         query_field=\"func_documentation_string\", code_field=\"func_code_string\"):\n",
        "    \"\"\"\n",
        "    ã‚¯ã‚¨ãƒªã¨ã‚³ãƒ¼ãƒ‰ã®ãƒšã‚¢ã‹ã‚‰ã€å€™è£œãƒ—ãƒ¼ãƒ«ã‚’ä½œæˆã—å„ç¨®è©•ä¾¡æŒ‡æ¨™ã‚’ç®—å‡ºã™ã‚‹é–¢æ•°ã§ã™ã€‚\n",
        "    ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã¯ dataset å†…ã®å®Ÿéš›ã®ã‚«ãƒ©ãƒ åã«åˆã‚ã›ã¦æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\n",
        "    \"\"\"\n",
        "    num_examples = min(len(dataset), max_examples)\n",
        "    all_codes = [dataset[i][code_field] for i in range(len(dataset))]\n",
        "    queries = []\n",
        "    correct_code_indices = []\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        ex = dataset[i]\n",
        "        query_text = ex[query_field]\n",
        "        queries.append(query_text)\n",
        "        correct_code_indices.append(i)\n",
        "\n",
        "    # ã‚³ãƒ¼ãƒ‰ã®åŸ‹ã‚è¾¼ã¿ã‚’äº‹å‰è¨ˆç®—\n",
        "    all_code_embeddings = []\n",
        "    print(\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®ã‚³ãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿ã‚’è¨ˆç®—ä¸­...\")\n",
        "    for code in all_codes[:num_examples]:\n",
        "        emb = get_cls_embedding(model, tokenizer, code, device)\n",
        "        all_code_embeddings.append(emb)\n",
        "    all_code_embeddings = np.concatenate(all_code_embeddings, axis=0)\n",
        "    print(\"ã‚³ãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿è¨ˆç®—å®Œäº†.\")\n",
        "\n",
        "    sim_matrices = []\n",
        "    print(\"å€™è£œã‚³ãƒ¼ãƒ‰ãƒ—ãƒ¼ãƒ«ã‚’ä½œæˆã—ã€é¡ä¼¼åº¦è¨ˆç®—ä¸­...\")\n",
        "    for i in range(num_examples):\n",
        "        query_embedding = get_cls_embedding(model, tokenizer, queries[i], device).reshape(1, -1)\n",
        "        candidate_pool_embeddings = []\n",
        "        candidate_pool_embeddings.append(all_code_embeddings[correct_code_indices[i]])\n",
        "        incorrect_code_indices = []\n",
        "        while len(incorrect_code_indices) < pool_size - 1:\n",
        "            rand_index = random.randint(0, num_examples - 1)\n",
        "            if rand_index != correct_code_indices[i] and rand_index not in incorrect_code_indices:\n",
        "                incorrect_code_indices.append(rand_index)\n",
        "        for incorrect_index in incorrect_code_indices:\n",
        "            candidate_pool_embeddings.append(all_code_embeddings[incorrect_index])\n",
        "        candidate_pool_embeddings = np.stack(candidate_pool_embeddings, axis=0)\n",
        "\n",
        "\n",
        "        sims = cosine_similarity(query_embedding, candidate_pool_embeddings)[0]\n",
        "        sim_matrices.append(sims)\n",
        "    sim_matrix = np.array(sim_matrices)\n",
        "    print(\"é¡ä¼¼åº¦è¨ˆç®—å®Œäº†.\")\n",
        "\n",
        "    metrics = calculate_metrics(sim_matrix)\n",
        "    return metrics\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    ######################################\n",
        "    # â‘¡ google/code_x_glue_tc_nl_code_search_adv ã‚’ç”¨ã„ãŸè©•ä¾¡ä¾‹\n",
        "    ######################################\n",
        "    print(\"\\ngoogle/code_x_glue_tc_nl_code_search_adv ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (Test) ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™...\")\n",
        "    tc_dataset = load_dataset(\"google/code_x_glue_tc_nl_code_search_adv\", split=\"test\")\n",
        "    subset_tc = tc_dataset.select(range(10000))  # å¿…è¦ã«å¿œã˜ã¦ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’èª¿æ•´\n",
        "\n",
        "    # ã“ã“ã§ã¯ã€ã‚¯ã‚¨ãƒªã¨ã—ã¦ \"docstring\"ã€ã‚³ãƒ¼ãƒ‰ã¨ã—ã¦ \"code\" ã®ã‚«ãƒ©ãƒ ã‚’åˆ©ç”¨\n",
        "    model_name1 = \"Shuu12121/CodeMorph-BERT\"\n",
        "    print(f\"\\n{model_name1} ã‚’ google/code_x_glue_tc_nl_code_search_adv ã§è©•ä¾¡ã—ã¾ã™ (å€™è£œãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º: 100)...\")\n",
        "    tokenizer1 = AutoTokenizer.from_pretrained(model_name1)\n",
        "    model1 = AutoModelForMaskedLM.from_pretrained(model_name1)\n",
        "    model1.to(device)\n",
        "    metrics_tc1 = evaluate_code_search(model1, tokenizer1, subset_tc, device, max_examples=10000, pool_size=100,\n",
        "                                       query_field=\"docstring\", code_field=\"code\")\n",
        "    print(f\"{model_name1} ã®è©•ä¾¡æŒ‡æ¨™ (google/code_x_glue_tc_nl_code_search_adv):\")\n",
        "    for name, value in metrics_tc1.items():\n",
        "        if isinstance(value, dict):\n",
        "            for k, v in value.items():\n",
        "                print(f\"  {name}@{k}: {v:.4f}\")\n",
        "        else:\n",
        "            print(f\"  {name}: {value:.4f}\")\n",
        "\n",
        "    # ä¾‹ã¨ã—ã¦ã€Microsoft CodeBERT ã§ã‚‚è©•ä¾¡ã™ã‚‹å ´åˆ\n",
        "    model_name2 = \"microsoft/codebert-base-mlm\"\n",
        "    print(f\"\\n{model_name2} ã‚’ google/code_x_glue_tc_nl_code_search_adv ã§è©•ä¾¡ã—ã¾ã™ (å€™è£œãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º: 100)...\")\n",
        "    tokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n",
        "    model2 = AutoModelForMaskedLM.from_pretrained(model_name2)\n",
        "    model2.to(device)\n",
        "    metrics_tc2 = evaluate_code_search(model2, tokenizer2, subset_tc, device, max_examples=10000, pool_size=100,\n",
        "                                       query_field=\"docstring\", code_field=\"code\")\n",
        "    print(f\"{model_name2} ã®è©•ä¾¡æŒ‡æ¨™ (google/code_x_glue_tc_nl_code_search_adv):\")\n",
        "    for name, value in metrics_tc2.items():\n",
        "        if isinstance(value, dict):\n",
        "            for k, v in value.items():\n",
        "                print(f\"  {name}@{k}: {v:.4f}\")\n",
        "        else:\n",
        "            print(f\"  {name}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwsga6SE_Mnj",
        "outputId": "b77b8f71-3509-4e10-80c6-f6e8562430cb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: cuda\n",
            "\n",
            "google/code_x_glue_tc_nl_code_search_adv ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (Test) ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™...\n",
            "\n",
            "Shuu12121/CodeMorph-BERT ã‚’ google/code_x_glue_tc_nl_code_search_adv ã§è©•ä¾¡ã—ã¾ã™ (å€™è£œãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º: 100)...\n",
            "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®ã‚³ãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿ã‚’è¨ˆç®—ä¸­...\n",
            "ã‚³ãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿è¨ˆç®—å®Œäº†.\n",
            "å€™è£œã‚³ãƒ¼ãƒ‰ãƒ—ãƒ¼ãƒ«ã‚’ä½œæˆã—ã€é¡ä¼¼åº¦è¨ˆç®—ä¸­...\n",
            "é¡ä¼¼åº¦è¨ˆç®—å®Œäº†.\n",
            "Shuu12121/CodeMorph-BERT ã®è©•ä¾¡æŒ‡æ¨™ (google/code_x_glue_tc_nl_code_search_adv):\n",
            "  mrr: 0.7788\n",
            "  recall@k@1: 0.7159\n",
            "  recall@k@5: 0.8509\n",
            "  recall@k@10: 0.8990\n",
            "  recall@k@50: 0.9847\n",
            "  recall@k@100: 1.0000\n",
            "  precision@k@1: 0.7159\n",
            "  precision@k@5: 0.7676\n",
            "  precision@k@10: 0.7741\n",
            "  precision@k@50: 0.7786\n",
            "  precision@k@100: 0.7788\n",
            "  ndcg@k@1: 0.7159\n",
            "  ndcg@k@5: 0.7885\n",
            "  ndcg@k@10: 0.8040\n",
            "  ndcg@k@50: 0.8236\n",
            "  ndcg@k@100: 0.8262\n",
            "  map: 0.7788\n",
            "  f1@k@1: 0.7159\n",
            "  f1@k@5: 0.7892\n",
            "  f1@k@10: 0.8005\n",
            "  f1@k@50: 0.8090\n",
            "  f1@k@100: 0.8095\n",
            "  r_precision: 0.7159\n",
            "  success_rate@k@1: 0.7159\n",
            "  success_rate@k@5: 0.8509\n",
            "  success_rate@k@10: 0.8990\n",
            "  success_rate@k@50: 0.9847\n",
            "  success_rate@k@100: 1.0000\n",
            "  query_coverage@k@1: 0.7159\n",
            "  query_coverage@k@5: 0.8509\n",
            "  query_coverage@k@10: 0.8990\n",
            "  query_coverage@k@50: 0.9847\n",
            "  query_coverage@k@100: 1.0000\n",
            "\n",
            "microsoft/codebert-base-mlm ã‚’ google/code_x_glue_tc_nl_code_search_adv ã§è©•ä¾¡ã—ã¾ã™ (å€™è£œãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º: 100)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/codebert-base-mlm were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®ã‚³ãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿ã‚’è¨ˆç®—ä¸­...\n",
            "ã‚³ãƒ¼ãƒ‰åŸ‹ã‚è¾¼ã¿è¨ˆç®—å®Œäº†.\n",
            "å€™è£œã‚³ãƒ¼ãƒ‰ãƒ—ãƒ¼ãƒ«ã‚’ä½œæˆã—ã€é¡ä¼¼åº¦è¨ˆç®—ä¸­...\n",
            "é¡ä¼¼åº¦è¨ˆç®—å®Œäº†.\n",
            "microsoft/codebert-base-mlm ã®è©•ä¾¡æŒ‡æ¨™ (google/code_x_glue_tc_nl_code_search_adv):\n",
            "  mrr: 0.7703\n",
            "  recall@k@1: 0.7205\n",
            "  recall@k@5: 0.8193\n",
            "  recall@k@10: 0.8781\n",
            "  recall@k@50: 0.9805\n",
            "  recall@k@100: 1.0000\n",
            "  precision@k@1: 0.7205\n",
            "  precision@k@5: 0.7565\n",
            "  precision@k@10: 0.7644\n",
            "  precision@k@50: 0.7700\n",
            "  precision@k@100: 0.7703\n",
            "  ndcg@k@1: 0.7205\n",
            "  ndcg@k@5: 0.7721\n",
            "  ndcg@k@10: 0.7912\n",
            "  ndcg@k@50: 0.8149\n",
            "  ndcg@k@100: 0.8180\n",
            "  map: 0.7703\n",
            "  f1@k@1: 0.7205\n",
            "  f1@k@5: 0.7722\n",
            "  f1@k@10: 0.7861\n",
            "  f1@k@50: 0.7965\n",
            "  f1@k@100: 0.7971\n",
            "  r_precision: 0.7205\n",
            "  success_rate@k@1: 0.7205\n",
            "  success_rate@k@5: 0.8193\n",
            "  success_rate@k@10: 0.8781\n",
            "  success_rate@k@50: 0.9805\n",
            "  success_rate@k@100: 1.0000\n",
            "  query_coverage@k@1: 0.7205\n",
            "  query_coverage@k@5: 0.8193\n",
            "  query_coverage@k@10: 0.8781\n",
            "  query_coverage@k@50: 0.9805\n",
            "  query_coverage@k@100: 1.0000\n"
          ]
        }
      ]
    }
  ]
}