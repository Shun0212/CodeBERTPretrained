# **CodeBERTもどき　事前学習**

## **はじめに**

**このリポジトリは CodeSearchNet の Python コードデータを使用して CodeBERT モデルを mlmを用いてゼロから事前学習するためのサンプルです.**

### **セットアップと初期化**

- **必要なライブラリのインストール**
- **Google Drive のマウントと使用 (Colab 使用時)**
- **作業環境の設定**

### **データ準備**

- **CodeSearchNet データセットから Python コードサンプルを抽出および処理**

### **トークナイザーの学習**

- **抽出したコードサンプルを用いた WordPiece ベースのトークナイザーの学習**

### **BERT モデルの初期化**

- **Masked Language Modeling (MLM) に適した BERT モデルの作成と設定**

### **データセットのトークナイズ**

- **コードサンプルをトークン化し、モデルの入力形式に変換**

### **トレーニングプロセス**

- **Hugging Face の Trainer API を使用してモデルを学習**
- **チェックポイントと評価を管理**

### **チェックポイント管理**

- **既存のチェックポイントをロードし、トレーニングの再開をサポート**

## **トレーニング手順**

### **1. セットアップ**

- **sentencepiece(使ってませんでした…そのうち消しておきます)、datasets、transformers などの依存ライブラリをインストール**
- **Google Drive のマウント (必要に応じて)**
- **作業ディレクトリの設定**

### **2. データ準備**

- **CodeSearchNet から Python コードデータをロード**
- **関数コード文字列 (func_code_string) を抽出**

### **3. トークナイザーの学習**

- **WordPiece トークナイザーの学習**
- **学習したトークナイザーを保存**

### **4. モデルの初期化**

- **BERT の構成を設定**
- **トークナイザーを使用してモデルを初期化**

### **5. データセットのトークナイズ**

- **学習したトークナイザーを使用**
- **トランケーションとパディングを適用**

### **6. モデルのテスト**

- **Masked Language Modeling (MLM) タスク**
  - **[MASK] でマスクした文字列を使って予測**
- **関数名予測タスク**
  - **関数名を [MASK] に置き換え、正しい名前を予測**

## **使用データセット**

- **データセット名:** CodeSearchNet
- **データタイプ:** Python コード
- **データセットサイズ:** 約 400,000 サンプル (train + validation + test)
- **主なカラム:**
  - `func_code_string`: Python 関数コード
  - `docstring`: 関数の説明 (未使用)

## **テスト結果**

### **Masked Language Modeling (MLM) タスク**

- **例: `def multiply(a, b): return a * [MASK]`**
- **予測結果:**
  - `b` (99.96%)
  - `a` (0.01%)
  - その他の誤り率: 0.03%

### **関数名(1トークン)予測タスク**

- **正解率:** 約 66.23%
- **誤り例:**
  - **入力:** `def [MASK](self, a, b): return a + b`
  - **正解:** `add`
  - **予測:** `sum`


